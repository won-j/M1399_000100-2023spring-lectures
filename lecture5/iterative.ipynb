{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(Matrix)\n",
    "library(Rlinsolve)  # iterative linear solvers\n",
    "library(SparseM) # for visualization\n",
    "library(lobstr)\n",
    "library(microbenchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Methods for Solving Linear Equations\n",
    "\n",
    "## Introduction\n",
    "\n",
    "So far we have considered direct methods for solving linear equations.    \n",
    "\n",
    "* **Direct methods** (flops fixed _a priori_) vs **iterative methods**:\n",
    "    - Direct method (GE/LU, Cholesky, QR, SVD): accurate. good for dense, small to moderate sized $\\mathbf{A}$  \n",
    "    - Iterative methods (Jacobi, Gauss-Seidal, SOR, conjugate-gradient, GMRES): accuracy depends on number of iterations. good for large, sparse, structured linear system, parallel computing, warm start (reasonable accuracy after, say, 100 iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: PageRank \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/PageRanks-Example.svg/400px-PageRanks-Example.svg.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "Source: [Wikepedia](https://en.wikipedia.org/wiki/PageRank)\n",
    "\n",
    "* $\\mathbf{A}  \\in \\{0,1\\}^{n \\times n}$ the connectivity matrix of webpages with entries\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\ta_{ij} = \\begin{cases}\n",
    "\t1 &  \\text{if page $i$ links to page $j$} \\\\\n",
    "\t0 & \\text{otherwise}\n",
    "\t\\end{cases}. \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "$n \\approx 10^9$ in May 2017.\n",
    "\n",
    "* $r_i = \\sum_j a_{ij}$ is the *out-degree* of page $i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Larry Page](https://en.wikipedia.org/wiki/PageRank) imagined a random surfer wandering on internet according to following rules:\n",
    "    - From a page $i$ with $r_i>0$\n",
    "        * with probability $p$, (s)he randomly chooses a link $j$ from page $i$ (uniformly) and follows that link to the next page  \n",
    "        * with probability $1-p$, (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page \n",
    "    - From a page $i$ with $r_i=0$ (a dangling page), (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page\n",
    "    \n",
    "* The process defines an $n$-state Markov chain, where each state corresponds to each page. \n",
    "$$\n",
    "    p_{ij} = (1-p)\\frac{1}{n} + p\\frac{a_{ij}}{r_i}\n",
    "$$\n",
    "with interpretation $a_{ij}/r_i = 1/n$ if $r_i = 0$.\n",
    "\n",
    "* Stationary distribution of this Markov chain gives the score (long term probability of visit) of each page.\n",
    "\n",
    "* Stationary distribution can be obtained as the top *left* eigenvector of the transition matrix $\\mathbf{P}=(p_{ij})$ corresponding to eigenvalue 1. \n",
    "$$\n",
    "    \\mathbf{x}^T\\mathbf{P} = \\mathbf{x}^T.\n",
    "$$\n",
    "Equivalently it can be cast as a linear equation.\n",
    "$$\n",
    "    (\\mathbf{I} - \\mathbf{P}^T) \\mathbf{x} = \\mathbf{0}.\n",
    "$$\n",
    "\n",
    "* You've got to solve a linear equation with $10^9$ variables!\n",
    "\n",
    "* GE/LU will take $2 \\times (10^9)^3/3/10^{12} \\approx 6.66 \\times 10^{14}$ seconds $\\approx 20$ million years on a tera-flop supercomputer!\n",
    "\n",
    "* Iterative methods come to the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and fixed point iteration\n",
    "\n",
    "* The key idea of iterative method for solving $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ is to **split** the matrix $\\mathbf{A}$ so that\n",
    "$$\n",
    "    \\mathbf{A} = \\mathbf{M} - \\mathbf{K}\n",
    "$$\n",
    "where $\\mathbf{M}$ is invertible and easy to invert.\n",
    "\n",
    "* Then $\\mathbf{A}\\mathbf{x} = \\mathbf{M}\\mathbf{x} - \\mathbf{K}\\mathbf{x} = \\mathbf{b}$ or\n",
    "$$\n",
    "    \\mathbf{x} = \\mathbf{M}^{-1}\\mathbf{K}\\mathbf{x} + \\mathbf{M}^{-1}\\mathbf{b}\n",
    "    .\n",
    "$$\n",
    "Thus a solution to $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ is a fixed point of iteration\n",
    "$$\n",
    "    \\mathbf{x}^{(t+1)} = \\mathbf{M}^{-1}\\mathbf{K}\\mathbf{x}^{(t)} + \\mathbf{M}^{-1}\\mathbf{b}\n",
    "    = \\mathbf{R}\\mathbf{x}^{(t)} + \\mathbf{c}\n",
    "    .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Under a suitable choice of $\\mathbf{R}$, i.e., splitting of $\\mathbf{A}$, the sequence $\\mathbf{x}^{(k)}$ generated by the above iteration converges to a solution $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$:\n",
    "\n",
    "\n",
    "> Let $\\rho(\\mathbf{R})=\\max_{i=1,\\dotsc,n}|\\lambda_i(\\mathbf{R})|$, where $\\lambda_i(\\mathbf{R})$ is the $i$th (complex) eigenvalue of $\\mathbf{R}$. When $\\mathbf{A}$ is invertible, the iteration $\\mathbf{x}^{(t+1)}=\\mathbf{R}\\mathbf{x}^{(t)} + \\mathbf{c}$ converges to $\\mathbf{A}^{-1}\\mathbf{b}$ for any $\\mathbf{x}^{(0)}$ if and only if  $\\rho(\\mathbf{R}) < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of vector norms\n",
    "\n",
    "A norm $\\|\\cdot\\|: \\mathbb{R}^n \\to \\mathbb{R}$ is defined by the following properties:\n",
    "\n",
    "1. $\\|\\mathbf{x}\\| \\ge 0$,\n",
    "2. $\\|\\mathbf{x}\\| = 0 \\iff \\mathbf{x} = \\mathbf{0}$,\n",
    "3. $\\|c\\mathbf{x}\\| = |c|\\|\\mathbf{x}\\|$ for all $c \\in \\mathbb{R}$,\n",
    "4. $\\|\\mathbf{x} + \\mathbf{y}\\| \\le \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|$.\n",
    "\n",
    "Typicall norms are\n",
    "* $\\ell_2$ (Euclidean) norm: $\\|\\mathbf{x}\\| = \\sqrt{\\sum_{i=1}^nx_i^2}$;\n",
    "* $\\ell_1$ norm: $\\|\\mathbf{x}\\| = \\sum_{i=1}^n|x_i|$;\n",
    "* $\\ell_{\\infty}$ (Manhatan) norm: $\\|\\mathbf{x}\\| = \\max_{i=1,\\dots,n}|x_i|$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix norms\n",
    "\n",
    "* Matrix $\\mathbf{A} \\in \\mathbb{R}^{n\\times n}$ can be considered as a vector in $\\mathbb{R}^{n^2}$. \n",
    "* However, it is convinient if a matrix norm is compatible with matrix multiplication.\n",
    "* In addition to the defining properties of a norm above, we require\n",
    "\n",
    "5. $\\|\\mathbf{A}\\mathbf{B}\\| \\le \\|\\mathbf{A}\\|\\|\\mathbf{B}\\|$.\n",
    "\n",
    "Typical matrix norms are\n",
    "* Frobenius norm: $\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j=1}^n a_{ij}^2} = \\sqrt{\\text{trace}(\\mathbf{A}\\mathbf{A}^T)}$;\n",
    "* Operator norm (induced by a vector norm $\\|\\cdot\\|$): $\\|\\mathbf{A}\\| = \\sup_{\\mathbf{y}\\neq\\mathbf{0}}\\frac{\\|\\mathbf{A}\\mathbf{y}\\|}{\\|\\mathbf{y}\\|}$.\n",
    "\n",
    "It can be shown that\n",
    "* $\\|\\mathbf{A}\\|_1 = \\max_{j=1,\\dotsc,n}\\sum_{i=1}^n|a_{ij}|$;\n",
    "* $\\|\\mathbf{A}\\|_{\\infty} = \\max_{i=1,\\dotsc,n}\\sum_{j=1}^n|a_{ij}|$;\n",
    "* $\\|\\mathbf{A}\\|_2 = \\sqrt{\\rho(\\mathbf{A}^T\\mathbf{A})}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence of iterative methods\n",
    "\n",
    "- Let $\\mathbf{x}^{\\star}$ be a solution of $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$.\n",
    "- Equivalently, $\\mathbf{x}^{\\star}$ satisfies $\\mathbf{x}^{\\star} = \\mathbf{R}\\mathbf{x}^{\\star} + \\mathbf{c}$.\n",
    "- Subtracting this from $\\mathbf{x}^{(t+1)} = \\mathbf{R}\\mathbf{x}^{(t)} + \\mathbf{c}$, we have\n",
    "$$\n",
    "    \\mathbf{x}^{(t+1)} - \\mathbf{x}^{\\star} = \\mathbf{R}(\\mathbf{x}^{(t)} - \\mathbf{x}^{\\star}).\n",
    "$$\n",
    "- Take a vector norm on both sides:\n",
    "$$\n",
    "    \\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^{\\star}\\| = \\|\\mathbf{R}(\\mathbf{x}^{(t)} - \\mathbf{x}^{\\star})\\| \n",
    "    \\le \\|\\mathbf{R}\\|\\|\\mathbf{x}^{(t)} - \\mathbf{x}^{\\star}\\|\n",
    "$$\n",
    "where $\\|\\mathbf{R}\\|$ is the induced operator norm.\n",
    "- Thus if $\\|\\mathbf{R}\\| < 1$ for a certain (induced) norm, then $\\mathbf{x}^{(t)} \\to \\mathbf{x}^{\\star}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem\n",
    "\n",
    "> The spectral radius $\\rho(\\mathbf{A})$ of a matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ satisfies\n",
    "$$\n",
    "    \\rho(\\mathbf{A}) \\le \\|\\mathbf{A}\\|\n",
    "$$\n",
    "for any operator norm. Furthermore, for any $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$  and $\\epsilon > 0$, there is an operator norm $\\|\\cdot\\|$ such that $\\|\\mathbf{A}\\| \\le \\rho(\\mathbf{A}) + \\epsilon$.\n",
    "\\\n",
    "In other words,\n",
    "$$\n",
    "    \\rho(\\mathbf{A}) = \\inf_{\\|\\cdot\\|: \\text{operator norm}} \\|\\mathbf{A}\\|.\n",
    "$$\n",
    "\n",
    "> Thus it is immediate to see\n",
    "$$\n",
    "    \\rho(\\mathbf{A}) < 1 \\iff \\|\\mathbf{A}\\| < 1\n",
    "$$\n",
    "for some operator norm.\n",
    "\n",
    "#### Proof\n",
    "If $\\lambda$ is an eigenvalue of $\\mathbf{A}$ with nonzero eigenvector $\\mathbf{v}$, then\n",
    "$$\n",
    "    \\|\\mathbf{A}\\mathbf{v}\\| = |\\lambda|\\|\\mathbf{v}\\|\n",
    "$$\n",
    "(for a vector norm). So \n",
    "$$\n",
    "    |\\lambda| = \\frac{\\|\\mathbf{A}\\mathbf{v}\\|}{\\|\\mathbf{v}\\|} \\le \\|\\mathbf{A}\\|,\n",
    "$$\n",
    "implying the first inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part, suppose $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$  and $\\epsilon > 0$ are given. Then there exists an (complex) upper triangular matrix $\\mathbf{T}$ and an invertible matrix $\\mathbf{S}$ such that\n",
    "$$\n",
    "    \\mathbf{A} = \\mathbf{S}\\mathbf{T}\\mathbf{S}^{-1}\n",
    "$$\n",
    "and the eigenvalues of $\\mathbf{A}$ coincides with the diagonal entries of $\\mathbf{T}$.\n",
    "(Schur decomposition).\n",
    "\n",
    "For $\\delta > 0$ consider a diagonal matrix $\\mathbf{D}(\\delta)=\\text{diag}(1, \\delta, \\delta^2, \\dotsc, \\delta^{n-1})$, which is invertible. Then \n",
    "\n",
    "$$\n",
    "\\mathbf{T}(\\delta) \n",
    "\\triangleq \\mathbf{D}(\\delta)^{-1}\\mathbf{T}\\mathbf{D}(\\delta)\n",
    "=\\mathbf{D}(\\delta)^{-1}\\mathbf{S}^{-1}\\mathbf{A}\\mathbf{S}\\mathbf{D}(\\delta) \n",
    "= (\\mathbf{S}\\mathbf{D}(\\delta))^{-1}\\mathbf{A}(\\mathbf{S}\\mathbf{D}(\\delta))\n",
    "$$\n",
    "\n",
    "is an upper triangular matrix with entries $(\\delta^{j-i}t_{ij})$ for $j \\ge i$. So the off-diagonal entries tends to 0 as $\\delta \\to 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_{\\delta} := \\|(\\mathbf{S}\\mathbf{D}(\\delta))^{-1}\\mathbf{x}\\|_{\\infty}\n",
    "$$\n",
    "defines a vector norm, which induces\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\|\\mathbf{A}\\|_{\\delta} \n",
    "&\\triangleq \\sup_{\\mathbf{x}\\neq\\mathbf{0}}\\frac{\\|\\mathbf{A}\\mathbf{x}\\|_{\\delta}}{\\|\\mathbf{x}\\|_{\\delta}}\n",
    "= \\sup_{\\mathbf{x}\\neq\\mathbf{0}}\\frac{\\|[\\mathbf{S}\\mathbf{D}(\\delta)]^{-1}\\mathbf{A}\\mathbf{x}\\|_{\\infty}}{\\|[\\mathbf{S}\\mathbf{D}(\\delta)]^{-1}\\mathbf{x}\\|_{\\infty}}\n",
    "\\\\\n",
    "&= \\|(\\mathbf{S}\\mathbf{D}(\\delta))^{-1}\\mathbf{A}(\\mathbf{S}\\mathbf{D}(\\delta))\\|_{\\infty} = \\|\\mathbf{T}(\\delta)\\|_{\\infty}\n",
    "= \\max_{i=1,\\dotsc,n}\\sum_{j=i}^n |t_{ij}|\\delta^{j-i}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Since for\n",
    "$\\mathbf{y} = [\\mathbf{S}\\mathbf{D}(\\delta)]^{-1}\\mathbf{x}$, there holds $\\mathbf{x} = [\\mathbf{S}\\mathbf{D}(\\delta)]\\mathbf{y}$ and\n",
    "\\\n",
    "$$\n",
    "\\sup_{\\mathbf{x}\\neq\\mathbf{0}}\\frac{\\|[\\mathbf{S}\\mathbf{D}(\\delta)]^{-1}\\mathbf{A}\\mathbf{x}\\|_{\\infty}}{\\|[\\mathbf{S}\\mathbf{D}(\\delta)]^{-1}\\mathbf{x}\\|_{\\infty}}\n",
    "= \n",
    "\\sup_{\\mathbf{y}\\neq\\mathbf{0}}\\frac{\\|[\\mathbf{S}\\mathbf{D}(\\delta)]^{-1}\\mathbf{A}[\\mathbf{S}\\mathbf{D}(\\delta)]\\mathbf{y}\\|_{\\infty}}{\\|[\\mathbf{S}\\mathbf{D}(\\delta)]^{-1}[\\mathbf{S}\\mathbf{D}(\\delta)]\\mathbf{y}\\|_{\\infty}}\n",
    "=\n",
    "\\sup_{\\mathbf{y}\\neq\\mathbf{0}}\\frac{\\|[\\mathbf{S}\\mathbf{D}(\\delta)]^{-1}\\mathbf{A}[\\mathbf{S}\\mathbf{D}(\\delta)]\\mathbf{y}\\|_{\\infty}}{\\|\\mathbf{y}\\|_{\\infty}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since the eigenvalues of $\\mathbf{A}$ coincides with $t_{ii}$, for each $\\epsilon>0$ we can take a sufficiently small $\\delta > 0$ so that\n",
    "$$\n",
    "    \\max_{i=1,\\dotsc,n}\\sum_{j=i}^n |t_{ij}|\\delta^{j-i} \\le \\rho(\\mathbf{A}) + \\epsilon.\n",
    "$$\n",
    "This implies $\\|\\mathbf{A}\\|_{\\delta} \\le \\rho(\\mathbf{A}) + \\epsilon$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implications\n",
    "\n",
    "* Recall\n",
    "> Let $\\rho(\\mathbf{R})=\\max_{i=1,\\dotsc,n}|\\lambda_i(\\mathbf{R})|$, where $\\lambda_i(\\mathbf{R})$ is the $i$th (complex) eigenvalue of $\\mathbf{R}$. When $\\mathbf{A}$ is invertible, the iteration $\\mathbf{x}^{(t+1)}=\\mathbf{R}\\mathbf{x}^{(t)} + \\mathbf{c}$ converges to $\\mathbf{A}^{-1}\\mathbf{b}$ for any $\\mathbf{x}^{(0)}$ if and only if  $\\rho(\\mathbf{R}) < 1$.\n",
    "\n",
    "* The *if* part is proved using the above Theorem. This part holds even if $\\mathbf{A}$ is singular; the limit may depend on $\\mathbf{x}^{(0)}$, but it is always a solution to $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$.\n",
    "\n",
    "* The *only if* part when $\\mathbf{A}$ is inverible (hence $\\mathbf{x}^{\\star}$ is unique):\n",
    "\\\n",
    "$$\n",
    "    \\mathbf{x}^{(t)} - \\mathbf{x}^{\\star} = \\mathbf{R}^t(\\mathbf{x}^{(0)} - \\mathbf{x}^{\\star}).\n",
    "$$\n",
    "\\\n",
    "If LHS converges to zero for any $\\mathbf{x}^{(0)}$, then $\\mathbf{R}^t \\to \\mathbf{0}$ as $t \\to \\infty$. This cannot occur if $|\\lambda_i(\\mathbf{R})| \\ge 1$ for some $i$: if $\\mathbf{x}^{(0)} = \\mathbf{v}_i + \\mathbf{x}^{\\star}$ for an eigenvector $\\mathbf{v}_i \\neq \\mathbf{0}$ such that $\\mathbf{R}\\mathbf{v}_i = \\lambda_i\\mathbf{v}_i$, then $\\| R^t(\\mathbf{x}^{(0)} - \\mathbf{x}^{\\star}) \\| = |\\lambda_i|^t\\|\\mathbf{v}_i\\| \\geq \\|\\mathbf{v}_i\\| \\neq 0$.\n",
    "    + Invertibility is necessary since otherwise we do not know if $\\mathbf{x}^{(t)}$ converges to a unique point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobi's method\n",
    "\n",
    "$$\n",
    "x_i^{(t+1)} = \\frac{b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(t)} - \\sum_{j=i+1}^n a_{ij} x_j^{(t)}}{a_{ii}}.\n",
    "$$\n",
    "\n",
    "\n",
    "* Split $\\mathbf{A} = \\mathbf{L} + \\mathbf{D} + \\mathbf{U}$ = (strictly lower triangular) + (diagonal) + (strictly upper triangular).\n",
    "\n",
    "\n",
    "* Take $\\mathbf{M}=\\mathbf{D}$ (easy to invert!) and $\\mathbf{K}=-(\\mathbf{L} + \\mathbf{U})$:\n",
    "$$\n",
    "    \\mathbf{D} \\mathbf{x}^{(t+1)} = - (\\mathbf{L} + \\mathbf{U}) \\mathbf{x}^{(t)} + \\mathbf{b},\n",
    "$$\n",
    "i.e., \n",
    "$$\n",
    "\t\\mathbf{x}^{(t+1)} = - \\mathbf{D}^{-1} (\\mathbf{L} + \\mathbf{U}) \\mathbf{x}^{(t)} + \\mathbf{D}^{-1} \\mathbf{b} = - \\mathbf{D}^{-1} \\mathbf{A} \\mathbf{x}^{(t)} + \\mathbf{x}^{(t)} + \\mathbf{D}^{-1} \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "* Convergence is guaranteed if $\\mathbf{A}$ is striclty row diagonally dominant: $|a_{ii}| > \\sum_{j\\neq i}|a_{ij}|$.\n",
    "\n",
    "* One round costs $2n^2$ flops with an **unstructured** $\\mathbf{A}$. Gain over GE/LU if converges in $o(n)$ iterations. \n",
    "\n",
    "* Saving is huge for **sparse** or **structured** $\\mathbf{A}$. By structured, we mean the matrix-vector multiplication $\\mathbf{A} \\mathbf{v}$ is fast ($O(n)$ or $O(n \\log n)$).\n",
    "    - Often the multiplication is implicit and $\\mathbf{A}$ is not even stored, e.g., finite difference: $(\\mathbf{A}\\mathbf{v})_i = v_i - v_{i+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Seidel method\n",
    "\n",
    "$$\n",
    "x_i^{(t+1)} = \\frac{b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(t+1)} - \\sum_{j=i+1}^n a_{ij} x_j^{(t)}}{a_{ii}}.\n",
    "$$\n",
    "\n",
    "* Split $\\mathbf{A} = \\mathbf{L} + \\mathbf{D} + \\mathbf{U}$ = (strictly lower triangular) + (diagonal) + (strictly upper triangular) as Jacobi.\n",
    "\n",
    "* Take $\\mathbf{M}=\\mathbf{D}+\\mathbf{L}$ (easy to invert, why?) and $\\mathbf{K}=-\\mathbf{U}$:\n",
    "$$\n",
    "(\\mathbf{D} + \\mathbf{L}) \\mathbf{x}^{(t+1)} = - \\mathbf{U} \\mathbf{x}^{(t)} + \\mathbf{b}\n",
    "$$\n",
    "i.e., \n",
    "$$\n",
    "\\mathbf{x}^{(t+1)} = - (\\mathbf{D} + \\mathbf{L})^{-1} \\mathbf{U} \\mathbf{x}^{(t)} + (\\mathbf{D} + \\mathbf{L})^{-1} \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "* Equivalent to\n",
    "$$\n",
    "\\mathbf{D}\\mathbf{x}^{(t+1)} = - \\mathbf{L} \\mathbf{x}^{(t+1)} - \\mathbf{U} \\mathbf{x}^{(t)} + \\mathbf{b}\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\mathbf{x}^{(t+1)} = \\mathbf{D}^{-1}(- \\mathbf{L} \\mathbf{x}^{(t+1)} - \\mathbf{U} \\mathbf{x}^{(t)} + \\mathbf{b})\n",
    "$$\n",
    "leading to the iteration.\n",
    "\n",
    "* \"Coordinate descent\" version of Jacobi.\n",
    "\n",
    "* Convergence is guaranteed if $\\mathbf{A}$ is striclty row diagonally dominant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successive over-relaxation (SOR)\n",
    "\n",
    "$$\n",
    "x_i^{(t+1)} = \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(t+1)} - \\sum_{j=i+1}^n a_{ij} x_j^{(t)} \\right)+ (1-\\omega) x_i^{(t)},\n",
    "$$\n",
    "\n",
    "* $\\omega=1$: Gauss-Seidel; $\\omega \\in (0, 1)$: underrelaxation; $\\omega > 1$: overrelaxation\n",
    "    - Relaxation in hope of faster convergence\n",
    "    \n",
    "* Split $\\mathbf{A} = \\mathbf{L} + \\mathbf{D} + \\mathbf{U}$ = (strictly lower triangular) + (diagonal) + (strictly upper triangular) as before.\n",
    "\n",
    "* Take $\\mathbf{M}=\\frac{1}{\\omega}\\mathbf{D}+\\mathbf{L}$ (easy to invert, why?) and $\\mathbf{K}=\\frac{1-\\omega}{\\omega}\\mathbf{D}-\\mathbf{U}$:\n",
    "$$\n",
    "\\begin{split}\n",
    "(\\mathbf{D} + \\omega \\mathbf{L})\\mathbf{x}^{(t+1)} &= [(1-\\omega) \\mathbf{D} - \\omega \\mathbf{U}] \\mathbf{x}^{(t)} +  \\omega \\mathbf{b} \n",
    "\\\\\n",
    "\\mathbf{D}\\mathbf{x}^{(t+1)} &= (1-\\omega) \\mathbf{D}\\mathbf{x}^{(t)} + \\omega ( -\\mathbf{U}\\mathbf{x}^{(t)} - \\mathbf{L}\\mathbf{x}^{(t+1)}  + \\mathbf{b} )\n",
    "\\\\\n",
    "\\mathbf{x}^{(t+1)} &= (1-\\omega)\\mathbf{x}^{(t)} + \\omega \\mathbf{D}^{-1} ( -\\mathbf{U}\\mathbf{x}^{(t)} - \\mathbf{L}\\mathbf{x}^{(t+1)}  + \\mathbf{b} )\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate gradient method\n",
    "\n",
    "* **Conjugate gradient and its variants are the top-notch iterative methods for solving huge, structured linear systems.**\n",
    "\n",
    "* Key idea: solving $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ is equivalent to minimizing the quadratic function $\\frac{1}{2} \\mathbf{x}^T \\mathbf{A} \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$ if $\\mathbf{A}$ *is positive definite*.\n",
    "\n",
    "    [Application to a fusion problem in physics](http://www.sciencedirect.com/science/article/pii/0021999178900980?via%3Dihub):\n",
    "\n",
    "| Method                                 | Number of Iterations |\n",
    "|----------------------------------------|----------------------|\n",
    "| Gauss-Seidel                           | 208,000              |\n",
    "| Block SOR methods                      | 765                  |\n",
    "| Incomplete Cholesky conjugate gradient | 25                   |\n",
    "\n",
    "\n",
    "* We defer the details of CG to the graduate-level Advanced Statistical Computing course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical examples\n",
    "\n",
    "The [`Rlinsolve`](https://cran.r-project.org/web/packages/Rlinsolve/index.html) package implements most commonly used iterative solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poisson matrix is a block tridiagonal matrix from (discretized) Poisson's equation $\\nabla^2\\psi = f$ in on the plane (with a certain boundary condition). This matrix is sparse, symmetric positive definite and has known eigenvalues.\n",
    "\n",
    "Reference:\n",
    "G. H. Golub and C. F. Van Loan, Matrix Computations, second edition, Johns Hopkins University Press, Baltimore, Maryland, 1989 (Section 4.5.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get1DPoissonMatrix <- function(n) {\n",
    "      Matrix::bandSparse(n, n, #dimensions\n",
    "                        (-1):1, #band, diagonal is number 0\n",
    "                        list(rep(-1, n-1), \n",
    "                        rep(4, n), \n",
    "                        rep(-1, n-1)))\n",
    "}\n",
    "get2DPoissonMatrix <- function(n) {  # n^n by n^n\n",
    "    T <- get1DPoissonMatrix(n)\n",
    "    eye <- Matrix::Diagonal(n)\n",
    "    N <- n * n  ## dimension of the final square matrix\n",
    "    ## construct two block diagonal matrices\n",
    "    D <- bdiag(rep.int(list(T), n))\n",
    "    O <- bdiag(rep.int(list(-eye), n - 1))\n",
    "\n",
    "    ## augment O and add them together with D\n",
    "    D +\n",
    "     rbind(cbind(Matrix(0, nrow(O), n), O), Matrix(0, n, N)) + \n",
    "     cbind(rbind(Matrix(0, n, ncol(O)), O), Matrix(0, N, n))\n",
    "}\n",
    "M <- get2DPoissonMatrix(10)\n",
    "SparseM::image(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- 50\n",
    "# Poisson matrix of dimension n^2=2500 by n^2, pd and sparse\n",
    "A <- get2DPoissonMatrix(n)  # sparse\n",
    "# dense matrix representation of A\n",
    "Afull <- as.matrix(A)\n",
    "# sparsity level\n",
    "nnzero(A) / length(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparseM::image(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage difference\n",
    "lobstr::obj_size(A) \n",
    "lobstr::obj_size(Afull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-vector muliplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly generated vector of length n^2\n",
    "set.seed(456) # seed\n",
    "b <- rnorm(n^2)\n",
    "# dense matrix-vector multiplication\n",
    "res <- microbenchmark::microbenchmark(Afull %*% b, A %*% b)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense solve via Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the Cholesky solution\n",
    "#Achol <- base::chol(Afull)   # awfully slow\n",
    "Achol <- Matrix::Cholesky(A)  # sparse Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two triangular solves;  2n^2 flops\n",
    "#y <- solve(t(Achol), b) \n",
    "#xchol <- solve(Achol, y)\n",
    "xchol <- solve(Achol, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobi solver\n",
    "\n",
    "It seems that Jacobi solver doesn't give the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xjacobi <- Rlinsolve::lsolve.jacobi(A, b)\n",
    "Matrix::norm(xjacobi$x - xchol, 'F') / Matrix::norm(xchol, 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Documentation](https://cran.r-project.org/web/packages/Rlinsolve/Rlinsolve.pdf) reveals that the default value of `maxiter` is 1000. A couple trial runs shows that 5000 Jacobi iterations are required to get an \"accurate enough\" solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xjacobi <- Rlinsolve::lsolve.jacobi(A, b, maxiter=5000)\n",
    "Matrix::norm(xjacobi$x - xchol, 'F') / Matrix::norm(xchol, 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xjacobi$iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Seidel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gauss-Seidel solution is fairly close to Cholesky solution after 5000 iters\n",
    "xgs <- Rlinsolve::lsolve.gs(A, b, maxiter=5000)\n",
    "Matrix::norm(xgs$x - xchol, 'F') / Matrix::norm(xchol, 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgs$iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetric SOR with ω=0.75\n",
    "xsor <- Rlinsolve::lsolve.ssor(A, b, w=0.75, maxiter=5000)\n",
    "Matrix::norm(xsor$x - xchol, 'F') / Matrix::norm(xchol, 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsor$iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conjugate gradient\n",
    "xcg <- Rlinsolve::lsolve.cg(A, b)\n",
    "Matrix::norm(xcg$x - xchol, 'F') / Matrix::norm(xchol, 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcg$iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A basic tenet in numerical analysis: \n",
    "\n",
    "> **The structure should be exploited whenever possible in solving a problem.** "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "143px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "443.5px",
    "left": "0px",
    "right": "796px",
    "top": "67px",
    "width": "164px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
