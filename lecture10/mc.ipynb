{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo integration\n",
    "\n",
    "## Naïve Monte Carlo\n",
    "\n",
    "### Goal\n",
    "\n",
    "To evaluate $\\int_A g(x) dx$, $g:\\mathbb{R}^d \\to \\mathbb{R}$.\n",
    "\n",
    "More generally, to evaluate $\\int_A g(x) w(x)dx$, $g:\\mathbb{R}^d \\to \\mathbb{R}$, for $w(x) > 0$ on $A \\subset \\mathbb{R}^d$ and $\\int_A w(x)dx < \\infty$.\n",
    "\n",
    "* cf. (Gaussian) quadrature: $\\int_a^b g(x) w(x)dx$, $g:\\mathbb{R} \\to \\mathbb{R}$, for $w(x) > 0$ on $(a, b)$ and $\\int_a^b w(x)dx < \\infty$.\n",
    "* Extension of quadrature methods to $d$ dimension required $n^d$ points (curse of dimensionality)\n",
    "* Quadrature methods works well mostly with smooth functions.\n",
    "\n",
    "After appropriate normalization,\n",
    "$$\n",
    "    \\int_A g(x) f(x)dx = E[g(X)], \\quad X \\sim F,\n",
    "$$\n",
    "where distribution $F$ has density $f$.\n",
    "\n",
    "By the law of large numbers,\n",
    "$$\n",
    "    E[g(X)] \\approx \\frac{1}{n}\\sum_{i=1}^ng(X_i),  \\quad X_i \\stackrel{iid}{\\sim} F\n",
    "$$\n",
    "for sufficiently large $n$. This method is called the **Monte Carlo method** for approximating the integral.\n",
    "\n",
    "The RHS of the above approximate equality, denoted by $\\hat{I}_n(f)$ is an *unbiased estimator* of the LHS.\n",
    "\n",
    "If in addition, if $\\text{Var}[g(X)]$ is finite or $g(x)$ is square integrable, the variance of this unbiased estimator is \n",
    "$$\n",
    "\\frac{1}{n}\\text{Var}[g(X)] = \\frac{\\int_A g^2(x)f(x)dx - (\\int_A g(x)f(x)dx)^2}{n}\n",
    "$$\n",
    "and the central limit theorem provides an (approximate) $100(1-\\alpha)$% confidence interval\n",
    "$$\n",
    "    \\left[\\hat{I}_n(f) - z_{1-\\alpha/2}\\frac{\\text{Sd}[g(X)]}{\\sqrt{n}}, \\hat{I}_n(f) + z_{1-\\alpha/2}\\frac{\\text{Sd}[g(X)]}{\\sqrt{n}} \\right].\n",
    "$$\n",
    "Since $\\text{Sd}[g(X)]$ is unknown in general, we may use\n",
    "$$\n",
    " \\widehat{\\text{Sd}[g(X)]} = \\left( \\frac{1}{n-1}\\sum_{i=1}^n[f(X_i) - I_n(f)]^2 \\right)^{1/2}.\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore we can measure the accuracy of the esitmator $\\hat{I}_n(f)$ by the relative error $\\widehat{\\text{Sd}[g(X)]} / [\\hat{I}_n(f)\\sqrt{n}]$.\n",
    "\n",
    "So the key feasures of MC itegration are\n",
    "\n",
    "1. The error estimate does not depende direcly on the dimsnsionality of the underlying space\n",
    "2. The error diminisihes at a slow rate of $O(n^{-1/2})$.\n",
    "\n",
    "On the other hand, quadrature methods (in one dimension) at the rate of $O(n^{-k})$ for $k \\ge 2$. If $n = m^d$ points are used in $d$ dimensions, then the error is $O(n^{-k/d})$. So the numerical merit of MC integration over quadrature methods is roughly for $d \\ge 4$.\n",
    "\n",
    "Nevertheless MC has an attraction of very simple implementation: just sample and average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "$$\n",
    "\\int_0^{\\infty} \\sqrt{x}e^{-x}dx = \\frac{\\sqrt{\\pi}}{2} \\approx 0.88627\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfn <- function(x) sqrt(x) * exp(-x)\n",
    "integrate(myfn, 0, Inf)    # (adaptive) quadrature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2020)\n",
    "nsamp <- 10000  # try 100000, too\n",
    "x <- rexp(nsamp, 1)   # X ~ Exp(1)\n",
    "xsqrt <- sqrt(x)\n",
    "MCest <- cumsum(xsqrt) / seq_along(xsqrt) # cumulative average\n",
    "RVdata <- data.frame(nsamp = seq_along(xsqrt), est = MCest)\n",
    "ggplot(RVdata) + geom_line(aes(x = nsamp, y = est), col=\"red\", lwd=1.2) +\n",
    "    geom_hline(yintercept = sqrt(pi) * 0.5, col=\"dark blue\", lwd=1) +\n",
    "    xlab(\"Sample size\") + ylab(\"MC integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "$$\n",
    "\\int_0^{1} \\frac{1}{\\sqrt{x}} = 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfn <- function(x) 1 / sqrt(x) \n",
    "integrate(myfn, 0, 1)    # (adaptive) quadrature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2020)\n",
    "nsamp <- 10000  # try 100000, too\n",
    "x <- runif(nsamp)   # X ~ unif(0, 1)\n",
    "xsqrtinv <- 1 / sqrt(x)\n",
    "MCest <- cumsum(xsqrtinv) / seq_along(xsqrtinv) # cummulative average\n",
    "RVdata <- data.frame(nsamp = seq_along(xsqrtinv), est = MCest)\n",
    "ggplot(RVdata) + geom_line(aes(x = nsamp, y = est), col=\"red\", lwd=1.2) +\n",
    "    geom_hline(yintercept = 2, col=\"dark blue\", lwd=1) +\n",
    "    xlab(\"Sample size\") + ylab(\"MC integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of this MC integration depends on \n",
    "$$\n",
    "    \\text{Var}[1/\\sqrt{X}] = E[1 / X] - E[1 / \\sqrt{X}]^2 = \\int_0^1 \\frac{1}{x}dx - 2^2 = \\infty\n",
    "$$\n",
    "\n",
    "So the naïve MC is not a good estmator of the integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2020)\n",
    "nsim <- 1000\n",
    "nsamp <- 10000  # try 100000, too\n",
    "x <- runif(nsamp * nsim)   # X ~ unif(0, 1)\n",
    "xsqrtinv <- 1 / sqrt(x)\n",
    "MCest <- apply(matrix(xsqrtinv, nrow = nsim), 1, mean)\n",
    "MCerr <- var(MCest)\n",
    "RVdata <- data.frame(MCest = MCest)\n",
    "ggplot(RVdata, aes(x = MCest)) + geom_histogram(binwidth = 0.01, col=\"black\", fill=\"white\") +\n",
    "    stat_function(\n",
    "        fun  = function(x, mean, sd, n, bw) dnorm(x = x, mean = mean, sd = sd) * n * bw,\n",
    "        args = c(mean = mean(MCest), sd = sqrt(MCerr), n = nsim, bw = 0.01),\n",
    "        lwd  = 1.2, col = \"dark red\"\n",
    "    ) +\n",
    "    xlim(c(1.9, 2.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better approach is change of variable: let $t = \\sqrt{x}$. Then $dt = \\frac{1}{2\\sqrt{x}}dx$ and,\n",
    "$$\n",
    "    \\int_0^1 \\frac{1}{\\sqrt{x}}dx = \\int_0^1 2dt\n",
    "$$\n",
    "\n",
    "Thus the MC estimate of the RHS is $\\frac{1}{n}\\sum_{i=1}^n 2 = 2$, exact.\n",
    "\n",
    "In general, if integrand gets closer to a constant, then  MC gets more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digression: Buffon's needle\n",
    "\n",
    "> A table is ruled with equidistant parallel lines a distance $D$ apart. A needle of length $L$, where $L \\le D$, is randomly thrown on the table. What is the probability that the needle will intersect one of the lines (the other possibility being that the needle will be completely contained in the strip between two lines)?\n",
    "\n",
    "\n",
    "![Buffon's needle](./buffon.png)\n",
    "\n",
    "* Let $X$ be the distance from the needle to the nearest parallel line.\n",
    "\n",
    "* Let $\\Theta$ be the angle between the needle and the projected line of length $X$.\n",
    "\n",
    "* The needle will intersect a line if the hypotenuse of the right triangle is less that $L/2$, or\n",
    "$X < \\frac{L}{2}\\cos\\Theta$.\n",
    "\n",
    "It is reasonable to assume that $X$ and $\\Theta$ are independent, uniformly distributed in $[0, D/2]$ and $[0, \\pi/2]$. Hence\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tP(X < \\frac{L}{2}\\cos\\Theta ) &= E\\left[\\mathbb{I}\\{X < \\frac{L}{2}\\cos\\Theta\\}\\right] \\\\\n",
    "    &= \\iint_{x<0.5L\\cos \\theta}f_X(x)f_{\\Theta}(\\theta) dxd\\theta \\\\\n",
    "\t&= \\frac{4}{\\pi D}\\int_0^{\\pi/2}\\int_0^{0.5L\\cos\\theta} dxd\\theta \\\\\n",
    "\t&= \\frac{4}{\\pi D}\\int_0^{\\pi/2}\\frac{L}{2}\\cos\\theta d\\theta\n",
    "\t= \\frac{2L}{\\pi D}\n",
    "\t.\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "Italian mathematician Mario Lazzarini (1901) used this result to estimate the value of $\\pi$. How?\n",
    "\n",
    "* Set $D=1$, $L = D/2 = 1/2$.\n",
    "* If in $n$ trials, $k$ hits occur, then $\\pi \\approx n/k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2020)\n",
    "nsamp <- 100000  # try 500000, too\n",
    "x <- runif(nsamp) * 0.5   # X ~ unif(0, 1/2)\n",
    "thet <- runif(nsamp) * pi / 2   # theta ~ unif(0, pi/2)\n",
    "numhits <- (x < 0.25 * cos(thet))\n",
    "pihat <- seq_along(numhits) / cumsum(numhits) \n",
    "RVdata <- data.frame(nsamp = seq_along(numhits), est = pihat)\n",
    "tail(RVdata)\n",
    "ggplot(RVdata) + geom_line(aes(x = nsamp, y = est), col=\"red\", lwd=1.2) +\n",
    "    geom_hline(yintercept = pi, col=\"dark blue\", lwd=1) +\n",
    "    xlab(\"Sample size\") + ylab(\"Estimate\") + ylim(3.0, 3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance reduction methods\n",
    "\n",
    "We live with the $O(n^{-1/2})$ convergence rate of MC, and attempt to reduce the constant factor, i.e., the $\\sqrt{\\text{Var}[g(X)]}$ part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance sampling\n",
    "\n",
    "In evaluating\n",
    "$$\n",
    "I = \\int_{-\\infty}^{\\infty} g(x) f(x)dx = E_f[g(X)],\n",
    "$$\n",
    "consider instead\n",
    "$$\n",
    "I = \\int_{-\\infty}^{\\infty} g(x) f(x)dx = \\int_{-\\infty}^{\\infty} \\frac{g(x)f(x)}{h(x)} h(x)dx\n",
    "= E_h\\left[\\frac{g(X)f(X)}{h(X)}\\right]\n",
    "$$\n",
    "for another density $h(x)$ such that $h(x) > 0$ whenever $g(x)f(x) \\neq 0$.\n",
    "\n",
    "If it is easy to sample from the distribution with density $h(x)$, then we may estimate $I$ by\n",
    "$$\n",
    "\\hat{I}_{\\text{imp}} = \\frac{1}{n}\\sum_{i=1}^n\\frac{g(X_i)f(X_i)}{h(X_i)},\n",
    "\\quad\n",
    "X_i \\stackrel{iid}{\\sim} h\n",
    "$$\n",
    "instead of the simple estimator\n",
    "$$\n",
    "\\hat{I}_{\\text{naive}} = \\frac{1}{n}\\sum_{i=1}^n f(X_i), \n",
    "\\quad\n",
    "X_i \\stackrel{iid}{\\sim} f\n",
    ".\n",
    "$$\n",
    "\n",
    "It is obvious that both $\\hat{I}_{\\text{imp}}$ and $\\hat{I}_{\\text{naive}}$ are  unbiased estimators of $I$.\n",
    "\n",
    "The variance of $\\hat{I}_{\\text{imp}}$ is smaller than that of $\\hat{I}_{\\text{naive}}$ if and only if\n",
    "$$\n",
    "    E_h\\left[\\left(\\frac{g(X)f(X)}{h(X)}\\right)^2\\right] \\le E_f[g^2(X)]\n",
    "$$\n",
    "or\n",
    "$$\n",
    "    \\int_{-\\infty}^{\\infty}\\left(\\frac{g(x)f(x)}{h(x)}\\right)^2 h(x)dx\n",
    "    \\le \n",
    "    \\int_{-\\infty}^{\\infty}g^2(x) f(x) dx\n",
    "$$\n",
    "\n",
    "First note that\n",
    "\\begin{align*}\n",
    "    \\left(\\int_{-\\infty}^{\\infty}|g(x)|f(x)dx\\right)^2 \n",
    "    &=\n",
    "    \\left(\\int_{-\\infty}^{\\infty} \\frac{|g(x)|f(x)}{\\sqrt{h(x)}}\\sqrt{h(x)}dx \\right)^2 \\\\\n",
    "    &\\le\n",
    "    \\int_{-\\infty}^{\\infty} \\frac{g^2(x)f^2(x)}{h(x)} dx \\int_{-\\infty}^{\\infty}h(x)dx \\\\\n",
    "    &=\n",
    "    \\int_{-\\infty}^{\\infty} \\left(\\frac{g(x)f(x)}{h(x)}\\right)^2h(x) dx \\cdot (1) \\\\\n",
    "    &= E\\left[\\left(\\frac{g(X)f(X)}{h(X)}\\right)^2\\right]\n",
    "    ,\n",
    "\\end{align*}\n",
    "where the inequality is Cauchy-Schwarz. Equality holds if and only if $\\frac{|g(x)|f(x)}{\\sqrt{h(x)}}= c\\sqrt{h(x)}$ for some $c > 0$ for all $x$, i.e., $h(x) \\propto |g(x)|f(x)$.\n",
    "\n",
    "So if we choose $h(x) = \\frac{|g(x)|f(x)}{\\int |g(u)|f(u)du}$, then\n",
    "\\begin{align*}\n",
    "    \\int_{-\\infty}^{\\infty}\\left(\\frac{g(x)f(x)}{h(x)}\\right)^2 h(x)dx\n",
    "    &=\n",
    "    \\left(\\int_{-\\infty}^{\\infty}|g(x)|f(x)dx\\right)^2 \\\\\n",
    "    &\\le\n",
    "    \\int_{-\\infty}^{\\infty} g^2(x)f(x)dx \\int_{-\\infty}^{\\infty}f(x)dx \\\\\n",
    "    &= \n",
    "    \\int_{-\\infty}^{\\infty} g^2(x)f(x)dx\n",
    "    .\n",
    "\\end{align*}\n",
    "The middle inequality is again Cauchy-Schwarz. Equality holds if and only if $|g(x)|\\sqrt{f(x)} = c\\sqrt{f(x)}$ for some $c \\ge 0$, or $|g(x)|$ is constant.\n",
    "\n",
    "\n",
    "* The minimum variance of $\\hat{I}_{\\text{imp}}$ is $\\frac{1}{n}[(E_f|g(X)|)^2 - I^2]$.\n",
    "    - If $g$ is nonnegative, then the variance can be zero!\n",
    "\n",
    "* However, $\\int |g(u)|f(u)du$ is unknown.\n",
    "\n",
    "* The above argument tells us that if we choose $h(x)$ close to be proportional to $|g(x)|f(x)$, then the variance is strictly reduced. <!--unless $g(x)$ is uninteresting.-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: normal probability\n",
    "\n",
    "Suppose we want to estimate the tail probability of standard normal:\n",
    "$$\n",
    "\\pi(c) = \\int_c^{\\infty}\\phi(x)dx = 1 - \\Phi(c) = \\Phi(-c),\n",
    "$$\n",
    "where $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/x}$ and $\\Phi$ is the cdf of standard normal.\n",
    "\n",
    "1. Naive MC\n",
    "\n",
    "We can write this probability as \n",
    "$$\n",
    "    \\pi(c) = \\int_{-\\infty}^{\\infty}\\mathbb{I}_{\\{x > c\\}}\\phi(x)dx.\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\hat{\\pi}_{\\text{naive}}(c) = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{I}_{\\{Z_i > c\\}},\n",
    "    \\quad\n",
    "    Z_i \\stackrel{iid}{\\sim} N(0, 1)\n",
    "    .\n",
    "$$\n",
    "Obviously, $E[\\hat{\\pi}_{\\text{naive}}(c)] = \\pi(c)$ and $\\text{Var}[\\hat{\\pi}_{\\text{naive}}(c)] = \\frac{1}{n}\\pi(c)( 1 - \\pi(c))$.\n",
    "\n",
    "2. Importance sampling\n",
    "\n",
    "We can alternatively write the probability $\\pi(c)$ as \n",
    "$$\n",
    "    \\pi(c) = \\int_{-\\infty}^{\\infty}\\mathbb{I}_{\\{x > c\\}}\\frac{\\phi(x)}{\\phi(x-c)}\\phi(x-c)dx\n",
    "    = \\int_{-\\infty}^{\\infty}\\mathbb{I}_{\\{x > c\\}}\\exp\\left(-cx + \\frac{1}{2}c^2\\right)\\phi(x-c)dx\n",
    "$$\n",
    "Hence\n",
    "\\begin{align*}\n",
    "    \\hat{\\pi}_{\\text{imp}}(c) &= \\frac{e^{c^2/2}}{n}\\sum_{i=1}^n \\mathbb{I}_{\\{X_i > c\\}}\\exp(-cX_i),\n",
    "    \\quad\n",
    "    X_i \\stackrel{iid}{\\sim} N(c, 1) \\\\\n",
    "    &= \\frac{e^{c^2/2}}{n}\\sum_{i=1}^n \\mathbb{I}_{\\{Z_i > 0\\}}\\exp(-c[Z_i + c]),\n",
    "    \\quad\n",
    "    Z_i \\stackrel{iid}{\\sim} N(0, 1) \\\\\n",
    "    .\n",
    "\\end{align*}\n",
    "\n",
    "It is clear $E[\\hat{\\pi}_{\\text{imp}}(c)] = \\pi(c)$. To evaluate the variance, note that $\\hat{\\pi}_{\\text{imp}}(c) = \\frac{1}{n}\\sum_{i=1}^n e^{c^2/2}Y_i$, where $Y_i \\stackrel{d}{=} \\mathbb{I}_{\\{X > c\\}}\\exp(-cX)$ for $X \\sim N(c,1)$.\n",
    "Hence\n",
    "\\begin{align*}\n",
    "    E[Y_i^2] = E[\\mathbb{I}_{\\{X > c\\}}\\exp(-2cX)]\n",
    "    &= \\int_c^{\\infty}\\exp(-2cx)\\phi(x-c)dx \\\\\n",
    "    &= \\int_c^{\\infty}\\exp(-2cx)\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(x-c)^2\\right)dx \\\\\n",
    "    &= \\int_c^{\\infty}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(x+c)^2\\right)dx \\\\\n",
    "    &= \\int_{2c}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}z^2\\right)dz \\\\\n",
    "    &= 1 - \\Phi(2c) = \\Phi(-2c).\n",
    "\\end{align*}\n",
    "Therefore,\n",
    "$$\n",
    "    \\text{Var}[\\hat{\\pi}_{\\text{imp}}(c)] = \\frac{1}{n}\\left(e^{c^2}\\Phi(-2c) - [\\pi(c)]^2\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2020)\n",
    "nsamp <- 3000\n",
    "\n",
    "z <- rnorm(nsamp)   # standard normal sample\n",
    "\n",
    "cvals <- seq(3.0, 7.0, by=0.1)\n",
    "nsim <- length(cvals)\n",
    "\n",
    "naiveMC <- numeric(nsim)\n",
    "naiveSD <- numeric(nsim)\n",
    "impMC <- numeric(nsim)\n",
    "impSD <- numeric(nsim)\n",
    "varratio <- numeric(nsim)\n",
    "\n",
    "truepi <- pnorm(-cvals)  # true values\n",
    "for (k in seq_along(cvals)) {\n",
    "    cval <- cvals[k]\n",
    "    indic <- (z > cval)\n",
    "    naiveMC[k] <- mean(indic)\n",
    "    naiveSD[k] <- sd(indic) / sqrt(nsim)\n",
    "    indic2 <- exp(0.5 * cval^2 ) * (z > 0) * exp(-cval * (z + cval))\n",
    "    impMC[k] <- mean(indic2)\n",
    "    impSD[k] <- sd(indic2) / sqrt(nsim)\n",
    "    varratio <- (impSD[k] / naiveSD[k])^2\n",
    "}\n",
    "\n",
    "RVdata <- data.frame(cvals, truepi, naiveMC, impMC, varratio)\n",
    "ggplot(RVdata, aes(x = cvals, y = truepi)) +\n",
    "    geom_line(col = \"black\", linetype = 3, lwd = 1.2) +\n",
    "    geom_line(aes(x = cvals, y = naiveMC), col = \"red\", linetype = 1, lwd = 1) +\n",
    "    geom_line(aes(x = cvals, y = impMC), col = \"green\", linetype = 2, lwd = 1) +\n",
    "    ylim(0, 0.001) +\n",
    "    xlab(\"c\") + ylab(\"MC estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(RVdata) + \n",
    "    geom_line(aes(x = cvals, y = naiveSD), col = \"red\", linetype = 1, lwd = 1) +\n",
    "    geom_line(aes(x = cvals, y = impSD), col = \"green\", linetype = 2, lwd = 1) +\n",
    "    xlab(\"c\") + ylab(\"Standard Deviation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antithetic variable method\n",
    "\n",
    "If both $I_1$ and $I_2$ are unbiased estimators of integral $I$, then\n",
    "$$\n",
    "    \\text{Var}\\left[\\frac{I_1 + I_2}{2}\\right] \n",
    "    = \\frac{1}{4}\\text{Var}[I_1] + \\frac{1}{4}\\text{Var}[I_2] + \\frac{1}{2}\\text{Cov}(I_1, I_2)\n",
    "    .\n",
    "$$\n",
    "\n",
    "If $I_1$ and $I_2$ are negatively correlated, \n",
    "$$\n",
    "    \\text{Var}\\left[\\frac{I_1 + I_2}{2}\\right] \n",
    "    \\le \\frac{1}{4}\\text{Var}[I_1] + \\frac{1}{4}\\text{Var}[I_2]\n",
    "    .\n",
    "$$\n",
    "So if we can find pairs of unbiased estimators of the interal, then the variance of the sample mean is these esitmators will be smaller than that of independent estimators.\n",
    "\n",
    "**Proposition 1**. Suppose $X$ is a random variable and the functions $g_1(x)$ and $g_2(x)$ are both nonincreasing or both nondecreasing in $x$. If the random variables $g_1(X)$ and $g_2(X)$ have finite second moments, then\n",
    "$$\n",
    "    \\text{Cov}(g_1(X), g_2(X)) \\ge 0.\n",
    "$$\n",
    "*Proof*. Consider another random variable $Y$ that is i.i.d. to $X$. By monotonicity,\n",
    "$$\n",
    "    [g_1(X) - g_1(Y)][g_2(X) - g_2(Y)] \\ge 0.\n",
    "$$\n",
    "Hence, \n",
    "$$\n",
    "\\begin{split}\n",
    "    0 &\\le E[g_1(X) - g_1(Y)][g_2(X) - g_2(Y)]  \\\\\n",
    "      &=E[g_1(X)g_2(X)] + E[g_1(Y)g_2(Y)] - E[g_1(X)]E[g_2(Y)] - E[g_1(Y)]E[g_2(X)]  \\\\\n",
    "      &= 2\\text{Cov}(g_1(X), g_2(X))\n",
    "      .\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Now suppose we want to evaluate the integral $I = \\int_{-\\infty}^{\\infty}g(x)f(x)dx$ for $g(x)$ increasing and $f(x)$ be a density with cdf $F(x)$. If $X \\sim F$, then $g(X)$ is an unbiased estimator of $I$. Such a random variable $X$ can be constructed by the inverse cdf method: for $U \\sim \\text{unif}(0, 1)$, it follows that $F^{-1}(U) \\sim F$, i.e., $I_1 = g(F^{-1}(U))$ is an unbiased estimator of $I$. Likewise, for the same random variable $U$, $I_2 = g(F^{-1}(1-U))$ is an unbiased estimator of $I$. Note that $g(F^{-1}(u))$ is nondecreasing in $u$ and $g(F^{-1}(1-u))$ is nonincreasing in $u$. By Proposition 1, $\\text{Cov}(I_1, I_2) \\le 0$. It follows that\n",
    "$$\n",
    "    \\hat{I}_{\\text{an}} = \\frac{1}{2n}\\sum_{i=1}^n[g(F^{-1}(U_i) + g(F^{-1}(1-U_i)],\n",
    "    \\quad\n",
    "    U_1, \\dotsc, U_n \\stackrel{iid}{\\sim} \\text{unif}(0,1) \n",
    "$$\n",
    "has smaller variance than the naïve MC with  the same sample size\n",
    "$$\n",
    "    \\hat{I}_{\\text{naive}} = \\frac{1}{n}\\sum_{i=1}^{n}g(F^{-1}(U_i))\n",
    "    .\n",
    "$$\n",
    "\n",
    "Another way to obtain antithetic variables is to exploit symmetry. If $f$ is symmeric around the point $\\mu$, then $X - \\mu \\stackrel{d}{=} \\mu - X$, or $2\\mu - X \\stackrel{d}{=} X$. Therefore, we can use $I_1 = g(X_i)$ and $I_2 = g(2\\mu - X_i)$ for $X_i \\stackrel{iid}{\\sim} F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: exponential integral\n",
    "\n",
    "$$\n",
    "    I = \\int_0^1 e^u du = e - 1 \\approx 1.71828\n",
    "$$\n",
    "\n",
    "1. Naïve MC\n",
    "$$\n",
    "    \\hat{I}_{\\text{naive}} = \\frac{1}{n}\\sum_{i=1}^n \\exp(U_i), \\quad\n",
    "    U_i \\stackrel{iid}{\\sim} \\text{unif}(0, 1)\n",
    "    .\n",
    "$$\n",
    "We have\n",
    "\\begin{align*}\n",
    "    E[\\hat{I}_{\\text{naive}}] &= E[e^{U}] = I \\\\\n",
    "    \\text{Var}[\\hat{I}_{\\text{naive}}] &= \\frac{1}{n}\\text{Var}[e^U] = \\frac{1}{n}\\left(\\frac{e^2-1}{2} - (e-1)^2\\right) \\approx \\frac{0.2420}{n}.\n",
    "\\end{align*}\n",
    "\n",
    "2. Antithetic MC\n",
    "$$\n",
    "    \\hat{I}_{\\text{an}} = \\frac{1}{2n}\\sum_{i=1}^n [\\exp(U_i) + \\exp(1 - U_i)], \\quad\n",
    "    U_i \\stackrel{iid}{\\sim} \\text{unif}(0, 1)\n",
    "    .\n",
    "$$\n",
    "\n",
    "We have \n",
    "\\begin{align*}\n",
    "    E[\\hat{I}_{\\text{as}}] &= \\frac{1}{2}(E[e^U] + E[e^{1-U}]) = I \\\\\n",
    "    \\text{Var}[\\hat{I}_{\\text{as}}] &= \\frac{1}{n}\\text{Var}\\left[\\frac{e^U + e^{1-U}}{2}\\right] = \\frac{1}{n}\\frac{\\text{Var}[e^U] + \\text{Var}[e^{1-U}] + 2\\text{Cov}(e^U, e^{1-U})}{4} \\\\\n",
    "    &= \\frac{1}{4n}(e^2 - 1 - 2(e-1)^2 +  2(e - (e-1)^2) \\\\\n",
    "    &\\approx \\frac{0.0039}{n}.\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummean <- function (x) cumsum(x) / seq_along(x)  # cumulative mean\n",
    "cumvar <- function (x, sd = FALSE) {  # cumulative variance or sd\n",
    "  x <- x - x[sample.int(length(x), 1)]  \n",
    "  n <- seq_along(x)\n",
    "  v <- (cumsum(x ^ 2) - cumsum(x) ^ 2 / n) / (n - 1)\n",
    "  if (sd) v <- sqrt(v)\n",
    "  v\n",
    "}\n",
    "\n",
    "set.seed(2020)\n",
    "\n",
    "nsamp <- 3000\n",
    "\n",
    "u <- runif(nsamp)   # uniform random sample\n",
    "\n",
    "\n",
    "naive <- exp(u)\n",
    "naiveMC <- cummean(naive)\n",
    "naiveSD <- cumvar(naive, sd = TRUE) / sqrt(seq_along(naive))\n",
    "antithetic <- (exp(u) + exp(1 - u)) * 0.5\n",
    "antiMC  <- mean(antithetic)\n",
    "antiSD  <- cumvar(antithetic, sd = TRUE) / sqrt(seq_along(antithetic))\n",
    "varratio <- (antiSD / naiveSD)^2\n",
    "\n",
    "\n",
    "RVdata <- data.frame(nsamp = seq_len(nsamp), naiveMC = naiveMC, \n",
    "                     naiveUpper = naiveMC + 2 * naiveSD, naiveLower = naiveMC - 2 * naiveSD,\n",
    "                     antiMC = antiMC,\n",
    "                     antiUpper = antiMC + 2 * antiSD, antiLower = antiMC - 2 * antiSD,\n",
    "                     varratio = varratio)\n",
    "ggplot(RVdata, aes(x = nsamp, y = naiveMC)) +\n",
    "    geom_line(col = \"red\", lwd = 1) +\n",
    "    #geom_point(shape = 16, col = \"black\", lwd = 1.7) +\n",
    "    geom_hline(yintercept = exp(1) - 1, col = \"dark blue\", lwd = 1) +\n",
    "    geom_line(aes(x = nsamp, y = naiveUpper), col = \"magenta\", lwd = 1.2, linetype = 2) +\n",
    "    geom_line(aes(x = nsamp, y = naiveLower), col = \"magenta\", lwd = 1.2, linetype = 2) +\n",
    "    ylim(1.5, 2.0) +\n",
    "    xlab(\"Sample Size\") + ylab(\"MC integral\") +\n",
    "    geom_line(aes(x = nsamp, y = antiMC), col = \"cyan\", lwd = 1) +\n",
    "    #geom_point(shape = 16, col = \"black\", lwd = 1.7) + \n",
    "    geom_line(aes(x = nsamp, y = antiUpper), col = \"green\", lwd = 1.2, linetype = 2) + \n",
    "    geom_line(aes(x = nsamp, y = antiLower), col = \"green\", lwd = 1.2, linetype = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(RVdata, aes(x = nsamp, y = varratio)) +\n",
    "    geom_point(shape = 16, col = \"black\", lwd = 1.7) + \n",
    "    geom_line(col = \"black\", linetype = 1) +\n",
    "    ylim(0.01, 0.02) +\n",
    "    xlab(\"Sample Size\") + ylab(\"Variance Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control variable method\n",
    "\n",
    "In estimating $I = E[f(X)]$, use another random variable $Y$ whose expectation $\\mu$ is known. Note that $\\hat{I} = f(X) - c(Y - \\mu)$ is an unbiased estimator of $I$ for any $c$. Its variance is\n",
    "$$\n",
    "    \\text{Var}[f(X) - c(Y - \\mu)] = \\text{Var}[f(X)] + c^2\\text{Var}[Y] -2c \\text{Cov}(f(X), Y)\n",
    "    ,\n",
    "$$\n",
    "which is minimized by\n",
    "$$\n",
    "    c^{\\star} = \\frac{\\text{Cov}(f(X), Y)}{\\text{Var}[Y]}\n",
    "    .\n",
    "$$\n",
    "and the minimum variance is\n",
    "$$\n",
    "    \\text{Var}[f(X) - c^{\\star}(Y - \\mu)] = \\text{Var}[f(X)] -\\frac{\\text{Cov}(f(X), Y)^2}{\\text{Var}[Y]}\n",
    "    .\n",
    "$$\n",
    "\n",
    "Thus if $f(X)$ and $Y$ are strongly correlated, then the variance reduces greatly.\n",
    "\n",
    "The random variable $Y$ is called the control variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: exponential integral\n",
    "\n",
    "$$\n",
    "    I = \\int_0^1 e^u du = e - 1 \\approx 1.71828\n",
    "$$\n",
    "\n",
    "1. Naïve MC\n",
    "$$\n",
    "    \\hat{I}_{\\text{naive}} = \\frac{1}{n}\\sum_{i=1}^n \\exp(U_i), \\quad\n",
    "    U_i \\stackrel{iid}{\\sim} \\text{unif}(0, 1)\n",
    "    .\n",
    "$$\n",
    "We have\n",
    "\\begin{align*}\n",
    "    E[\\hat{I}_{\\text{naive}}] &= E[e^{U}] = I \\\\\n",
    "    \\text{Var}[\\hat{I}_{\\text{naive}}] &= \\frac{1}{n}\\text{Var}[e^U] = \\frac{1}{n}\\left(\\frac{e^2-1}{2} - (e-1)^2\\right) \\approx \\frac{0.2420}{n}.\n",
    "\\end{align*}\n",
    "\n",
    "2. Antithetic MC\n",
    "$$\n",
    "    \\hat{I}_{\\text{an}} = \\frac{1}{2n}\\sum_{i=1}^n [\\exp(U_i) + \\exp(1 - U_i)], \\quad\n",
    "    U_i \\stackrel{iid}{\\sim} \\text{unif}(0, 1)\n",
    "    .\n",
    "$$\n",
    "We have \n",
    "\\begin{align*}\n",
    "    E[\\hat{I}_{\\text{as}}] &= \\frac{1}{2}(E[e^U] + E[e^{1-U}]) = I \\\\\n",
    "    \\text{Var}[\\hat{I}_{\\text{as}}] &= \\frac{1}{n}\\text{Var}\\left[\\frac{e^U + e^{1-U}}{2}\\right] = \\frac{1}{n}\\frac{\\text{Var}[e^U] + \\text{Var}[e^{1-U}] + 2\\text{Cov}(e^U, e^{1-U})}{4} \\\\\n",
    "    &= \\frac{1}{4n}(e^2 - 1 - 2(e-1)^2 +  2(e - (e-1)^2) \\\\\n",
    "    &\\approx \\frac{0.0039}{n}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "3. Control variable MC\n",
    "    - Use $U$ for a control variable.\n",
    "$$    \n",
    "    \\text{Cov}(e^U, U) = E[Ue^U] - E[U]E[e^U] = \\int_0^1 ue^u du - \\frac{1}{2}(e - 1) = \\frac{1}{2}(3 - e)\n",
    "    \\approx 0.14086\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "    c^{\\star} = \\frac{\\text{Cov}(e^U, U)}{\\text{Var}[U]} = \\frac{(3 - e)/2}{1/12} \\approx 1.6903\n",
    "$$\n",
    "and the control variable estimator is\n",
    "$$\n",
    "    \\hat{I}_{\\text{cv}} = \\frac{1}{n}\\sum_{i=1}^n\\left(\\exp(U_i) - c^{\\star}(U_i - 1/2)\\right)\n",
    "$$\n",
    "with variance\n",
    "$$\n",
    "    \\text{Var}[\\hat{I}_{\\text{cv}}] = \\frac{1}{n}\\text{Var}[e^U - c^{\\star}(U - 1/2)]\n",
    "    ,\n",
    "$$\n",
    "where\n",
    "\\begin{align*}\n",
    "\\text{Var}[e^U - c^{\\star}(U - 1/2)] &= \\text{Var}[e^U] -\\frac{\\text{Cov}(e^U, U)^2}{\\text{Var}[Y]} \\\\\n",
    "&= \\frac{4e - 3 - e^2}{2} - 12\\left(\\frac{3-e}{2}\\right)^2 \n",
    "\\approx 0.0039\n",
    ".\n",
    "\\end{align*}\n",
    "Therefore\n",
    "$$\n",
    "    \\hat{I}_{\\text{cv}} \\approx \\frac{0.0039}{n}\n",
    "    .\n",
    "$$\n",
    "\n",
    "If we can choose $c$ well (e.g., by regression), then the variance reduces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continued from the previous example\n",
    "\n",
    "#cstar <- 1.6903\n",
    "\n",
    "cvals <- seq(-0.4, 3.1, by = 0.1)\n",
    "nsim <- length(cvals)\n",
    "\n",
    "ctrlMC <- numeric(nsim)\n",
    "ctrlSD <- numeric(nsim)\n",
    "varrate <- numeric(nsim)\n",
    "\n",
    "for (k in seq_along(cvals)) {\n",
    "    cstar <- cvals[k]\n",
    "    ctrl <- exp(u) - cstar * (u - 0.5)\n",
    "    ctrlMC[k]  <- mean(ctrl)\n",
    "    ctrlSD[k]  <- sd(ctrl) / sqrt(nsamp)\n",
    "    varrate[k] <- (ctrlSD[k] / naiveSD[length(naiveSD)])^2\n",
    "}\n",
    "\n",
    "RVdata <- data.frame(c = cvals, \n",
    "                     ctrlMC = ctrlMC,\n",
    "                     ctrlUpper = ctrlMC + 2 * ctrlSD, ctrlLower = ctrlMC - 2 * ctrlSD,\n",
    "                     varratio = varrate)\n",
    "ggplot(RVdata, aes(x = c, y = ctrlMC)) +\n",
    "    geom_hline(yintercept = naiveMC[length(naiveMC)], col = \"red\", lwd = 1) +\n",
    "    #geom_point(shape = 16, col = \"black\", lwd = 1.7) +\n",
    "    geom_hline(yintercept = exp(1) - 1, col = \"dark blue\", lwd = 1) +\n",
    "    geom_hline(yintercept = naiveMC[length(naiveMC)] + 2 * naiveSD[length(naiveSD)], col = \"magenta\", lwd = 1.2, linetype = 2) +\n",
    "    geom_hline(yintercept = naiveMC[length(naiveMC)] - 2 * naiveSD[length(naiveSD)], col = \"magenta\", lwd = 1.2, linetype = 2) +\n",
    "    ylim(1.68, 1.76) +\n",
    "    xlab(\"c\") + ylab(\"MC integral\") +\n",
    "    geom_line(aes(x = c, y = ctrlMC), col = \"cyan\", lwd = 1) +\n",
    "    #geom_point(shape = 16, col = \"black\", lwd = 1.7) + \n",
    "    geom_line(aes(x = c, y = ctrlUpper), col = \"green\", lwd = 1.2, linetype = 2) + \n",
    "    geom_line(aes(x = c, y = ctrlLower), col = \"green\", lwd = 1.2, linetype = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(RVdata, aes(x = c, y = varratio)) +\n",
    "    geom_hline(yintercept = 1.0, col = \"red\", lwd = 1) +\n",
    "    geom_point(shape = 16, col = \"black\", lwd = 1.7) + \n",
    "    geom_line(col = \"black\", linetype = 1) +\n",
    "    ylim(0.0, 2.0) +\n",
    "    xlab(\"c\") + ylab(\"Variance Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "153px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "441.3333435058594px",
    "left": "0px",
    "right": "903.3333129882813px",
    "top": "140.6666717529297px",
    "width": "166px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
