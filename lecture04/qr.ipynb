{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR Decomposition\n",
    "\n",
    "* We learned Cholesky decomposition as **one** approach for solving linear regression.\n",
    "\n",
    "* Another approach for linear regression uses the QR decomposition.  \n",
    "    **This is how the `lm()` function in R does linear regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2020) # seed\n",
    "\n",
    "n <- 5\n",
    "p <- 3\n",
    "(X <- matrix(rnorm(n * p), nrow=n)) # predictor matrix\n",
    "(y <- rnorm(n))    # response vector\n",
    "\n",
    "# find the (minimum L2 norm) least squares solution\n",
    "lm(y ~ X - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to understand what is QR and how it is used for solving least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "* Assume $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ has full column rank. Necessarilly $n \\ge p$.\n",
    "\n",
    "* **Full QR decomposition**:  \n",
    "$$\n",
    "    \\mathbf{X} = \\mathbf{Q} \\mathbf{R},  \n",
    "$$\n",
    "where  \n",
    "    - $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{Q}^T \\mathbf{Q} = \\mathbf{Q}\\mathbf{Q}^T = \\mathbf{I}_n$. In other words, $\\mathbf{Q}$ is an orthogonal matrix.  \n",
    "        - First $p$ columns of $\\mathbf{Q}$ form an orthonormal basis of ${\\cal R}(\\mathbf{X})$ (**range** or column space of $\\mathbf{X}$)      \n",
    "        - Last $n-p$ columns of $\\mathbf{Q}$ form an orthonormal basis of ${\\cal N}(\\mathbf{X}^T)$ (**null space** of $\\mathbf{X}^T$)\n",
    "        - Recall that $\\mathcal{N}(\\mathbf{X}^T)=\\mathcal{R}(\\mathbf{X})^{\\perp}$ and $\\mathcal{R}(\\mathbf{X}) \\oplus \\mathcal{N}(\\mathbf{X}^T) = \\mathbb{R}^n$.\n",
    "    - $\\mathbf{R} \\in \\mathbb{R}^{n \\times p}$  is upper triangular with positive diagonal entries. \n",
    "        - The lower $(n-p)\\times p$ block of $\\mathbf{R}$ is $\\mathbf{0}$ (why?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **Reduced QR decomposition**:\n",
    "$$\n",
    "    \\mathbf{X} = \\mathbf{Q}_1 \\mathbf{R}_1,\n",
    "$$\n",
    "where\n",
    "    - $\\mathbf{Q}_1 \\in \\mathbb{R}^{n \\times p}$, $\\mathbf{Q}_1^T \\mathbf{Q}_1 = \\mathbf{I}_p$. In other words, $\\mathbf{Q}_1$ is a partially orthogonal matrix. Note $\\mathbf{Q}_1\\mathbf{Q}_1^T \\neq \\mathbf{I}_n$.\n",
    "    - $\\mathbf{R}_1 \\in \\mathbb{R}^{p \\times p}$  is an upper triangular matrix with positive diagonal entries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given QR decomposition $\\mathbf{X} = \\mathbf{Q} \\mathbf{R}$,\n",
    "    $$\n",
    "    \\mathbf{X}^T \\mathbf{X} = \\mathbf{R}^T \\mathbf{Q}^T \\mathbf{Q} \\mathbf{R} = \\mathbf{R}^T \\mathbf{R} = \\mathbf{R}_1^T \\mathbf{R}_1.\n",
    "    $$\n",
    "    - Once we have a (reduced) QR decomposition of $\\mathbf{X}$, we automatically have the Cholesky decomposition of the *Gram matrix*  $\\mathbf{X}^T \\mathbf{X}$.\n",
    "    \n",
    "### Least squares\n",
    "\n",
    "* Normal equation\n",
    "$$\n",
    "    \\mathbf{X}^T\\mathbf{X}\\beta = \\mathbf{X}^T\\mathbf{y}\n",
    "$$\n",
    "is equivalently written with reduced QR as\n",
    "$$\n",
    "    \\mathbf{R}_1^T\\mathbf{R}_1\\beta = \\mathbf{R}_1^T\\mathbf{Q}_1^T\\mathbf{y}\n",
    "$$\n",
    "\n",
    "* Since $\\mathbf{R}_1$ is invertible, we only need to solve the triangluar system\n",
    "$$\n",
    "    \\mathbf{R}_1\\beta = \\mathbf{Q}_1^T\\mathbf{y}\n",
    "$$\n",
    "Multiplication $\\mathbf{Q}_1^T \\mathbf{y}$ is done implicitly (see below).\n",
    "\n",
    "* This method is numerically more stable than directly solving the normal equation, since $\\kappa(\\mathbf{X}^T\\mathbf{X}) = \\kappa(\\mathbf{X})^2$!\n",
    "\n",
    "* In case we need standard errors, compute inverse of $\\mathbf{R}_1^T \\mathbf{R}_1$. This involves triangular solves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram-Schmidt procedure\n",
    "\n",
    "* Wait! Does $\\mathbf{X}$ always have a QR decomposition?\n",
    "    - Yes. It is equivalent to the [Gram-Schmidt procedure](https://en.wikipedia.org/wiki/Gramâ€“Schmidt_process) for basis orthonormalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assume $\\mathbf{X} = [\\mathbf{x}_1 | \\dotsb | \\mathbf{x}_p] \\in \\mathbb{R}^{n \\times p}$ has full column rank. That is,  $\\mathbf{x}_1,\\ldots,\\mathbf{x}_p$ are *linearly independent*.\n",
    "\n",
    "* Gram-Schmidt (GS) procedure produces nested orthonormal basis vectors $\\{\\mathbf{q}_1, \\dotsc, \\mathbf{q}_p\\}$ that spans $\\mathcal{R}(\\mathbf{X})$, i.e.,\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\text{span}(\\{\\mathbf{x}_1\\}) &= \\text{span}(\\{\\mathbf{q}_1\\}) \\\\\n",
    "    \\text{span}(\\{\\mathbf{x}_1, \\mathbf{x}_2\\}) &= \\text{span}(\\{\\mathbf{q}_1, \\mathbf{q}_2\\}) \\\\\n",
    "    & \\vdots \\\\\n",
    "    \\text{span}(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\dotsc, \\mathbf{x}_p\\}) &= \\text{span}(\\{\\mathbf{q}_1, \\mathbf{q}_2, \\dotsc, \\mathbf{q}_p\\}) \n",
    "\\end{split}\n",
    "$$\n",
    "and $\\langle \\mathbf{q}_i, \\mathbf{q}_j \\rangle = \\delta_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The algorithm:\n",
    "    0. Initialize $\\mathbf{q}_1 = \\mathbf{x}_1 / \\|\\mathbf{x}_1\\|_2$\n",
    "    0. For $k=2, \\ldots, p$, \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{v}_k &=& \\mathbf{x}_k - P_{\\text{span}(\\{\\mathbf{q}_1,\\ldots,\\mathbf{q}_{k-1}\\})}(\\mathbf{x}_k) = \\mathbf{x}_k -  \\sum_{j=1}^{k-1} \\langle \\mathbf{q}_j, \\mathbf{x}_k \\rangle \\cdot \\mathbf{q}_j \\\\\n",
    "\t\\mathbf{q}_k &=& \\mathbf{v}_k / \\|\\mathbf{v}_k\\|_2\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GS conducts reduced QR\n",
    "\n",
    "* $\\mathbf{Q} = [\\mathbf{q}_1 | \\dotsb | \\mathbf{q}_p]$. Obviously $\\mathbf{Q}^T \\mathbf{Q} = \\mathbf{I}_p$.\n",
    "\n",
    "* Where is $\\mathbf{R}$? \n",
    "    - Let $r_{jk} = \\langle \\mathbf{q}_j, \\mathbf{x}_k \\rangle$ for $j < k$, and $r_{kk} = \\|\\mathbf{v}_k\\|_2$.\n",
    "    - Re-write the above expression:\n",
    "$$\n",
    "    r_{kk} \\mathbf{q}_k = \\mathbf{x}_k -  \\sum_{j=1}^{k-1} r_{jk} \\cdot \\mathbf{q}_j\n",
    "$$\n",
    "        or\n",
    "$$\n",
    "    \\mathbf{x}_k = r_{kk} \\mathbf{q}_k +  \\sum_{j=1}^{k-1} r_{jk} \\cdot \\mathbf{q}_j\n",
    "    .\n",
    "$$    \n",
    "    - If we let $r_{jk} = 0$ for $j > k$, then $\\mathbf{R}=(r_{jk})$ is upper triangular and\n",
    "$$\n",
    "    \\mathbf{X} = \\mathbf{Q}\\mathbf{R}\n",
    "    .\n",
    "$$\n",
    " \n",
    "<img src=\"https://dsc-spidal.github.io/harp/img/daalAlgosNew/HarpIllustrations_QR.png\" width=\"800\" align=\"center\"/> \n",
    "  \n",
    "Source: <https://dsc-spidal.github.io/harp/docs/harpdaal/algorithms/>  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Gram-Schmidt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgs <- function(X) {  # not in-place\n",
    "    n <- dim(X)[1]\n",
    "    p <- dim(X)[2]    \n",
    "    Q <- Matrix(0, nrow=n, ncol=p, sparse=FALSE)\n",
    "    R <- Matrix(0, nrow=p, ncol=p, sparse=FALSE)\n",
    "    for (k in seq_len(p)) {\n",
    "        Q[, k] <- X[, k]\n",
    "        for (j in seq_len(k-1)) {\n",
    "            R[j, k] = sum(Q[, j] * X[, k])  # dot product\n",
    "            Q[, k] <- Q[, k] - R[j, k] * Q[, j]\n",
    "        }\n",
    "        R[k, k] <- Matrix::norm(Q[, k, drop=FALSE], \"F\")\n",
    "        Q[, k] <- Q[, k] / R[k, k]\n",
    "    }\n",
    "    list(Q=Q, R=R)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CGS is *unstable* (we lose orthogonality due to roundoff errors) when columns of $\\mathbf{X}$ are almost collinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e <- .Machine$double.eps\n",
    "(A <- t(Matrix(c(1, 1, 1, e, 0, 0, 0, e, 0, 0, 0, e), nrow=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res <- cgs(A)\n",
    "res$Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(res, t(Q) %*% Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Q` is hardly orthogonal.\n",
    "* Where exactly does the problem occur? (HW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Gram-Schmidt\n",
    "\n",
    "* The algorithm:\n",
    "    0. Initialize $\\mathbf{q}_1 = \\mathbf{x}_1 / \\|\\mathbf{x}_1\\|_2$\n",
    "    0. For $k=2, \\ldots, p$, \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{v}_k &=& \\mathbf{x}_k - P_{\\text{span}(\\{\\mathbf{q}_1,\\ldots,\\mathbf{q}_{k-1}\\})}(\\mathbf{x}_k) = \\mathbf{x}_k -  \\sum_{j=1}^{k-1} \\langle \\mathbf{q}_j, \\mathbf{x}_k \\rangle \\cdot \\mathbf{q}_j \\\\\n",
    "    &=&  \\mathbf{x}_k -  \\sum_{j=1}^{k-1} \\left\\langle \\mathbf{q}_j, \\mathbf{x}_k - \\sum_{l=1}^{j-1}\\langle \\mathbf{q}_l, \\mathbf{x}_k \\rangle \\mathbf{q}_l \\right\\rangle \\cdot \\mathbf{q}_j \\\\\n",
    "\t\\mathbf{q}_k &=& \\mathbf{v}_k / \\|\\mathbf{v}_k\\|_2\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgs <- function(X) { # not in-place\n",
    "    n <- dim(X)[1]\n",
    "    p <- dim(X)[2]\n",
    "    R <- Matrix(0, nrow=p, ncol=p)\n",
    "    for (k in seq_len(p)) {\n",
    "        for (j in seq_len(k-1)) {\n",
    "            R[j, k] <- sum(X[, j] * X[, k])  # dot product\n",
    "            X[, k] <- X[, k] - R[j, k] * X[, j]\n",
    "        }\n",
    "        R[k, k] <- Matrix::norm(X[, k, drop=FALSE], \"F\")\n",
    "        X[, k] <- X[, k] / R[k, k]\n",
    "    }\n",
    "    list(Q=X, R=R)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\mathbf{X}$ is overwritten by $\\mathbf{Q}$ and $\\mathbf{R}$ is stored in a separate array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mres <- mgs(A)\n",
    "mres$Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(mres, t(Q) %*% Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So MGS is more stable than CGS. However, even MGS is not completely immune to instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(B <- t(Matrix(c(0.7, 1/sqrt(2), 0.7 + e, 1/sqrt(2)), nrow=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mres2 <- mgs(B)\n",
    "mres2$Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(mres2, t(Q) %*% Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Q` is hardly orthogonal.\n",
    "* Where exactly does the problem occur? (HW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Computational cost of CGS and MGS is $\\sum_{k=1}^p 4n(k-1) \\approx 2np^2$.\n",
    "\n",
    "* There are 3 algorithms to compute QR: (modified) Gram-Schmidt, Householder transform, (fast) Givens transform.\n",
    "\n",
    "    In particular, the **Householder transform** for QR is implemented in LAPACK and thus used in R, Matlab, and Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR by Householder transform\n",
    "\n",
    "<img src=\"http://www-history.mcs.st-andrews.ac.uk/BigPictures/Householder_2.jpeg\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "[Alston Scott Householder (1904-1993)](https://en.wikipedia.org/wiki/Alston_Scott_Householder)\n",
    "\n",
    "* **This is the algorithm for solving linear regression in R**.\n",
    "\n",
    "* Assume again $\\mathbf{X} = [\\mathbf{x}_1 | \\dotsb | \\mathbf{x}_p] \\in \\mathbb{R}^{n \\times p}$ has full column rank.\n",
    "\n",
    "* Gram-Schmidt can be understood as:\n",
    "$$\n",
    "    \\mathbf{X}\\mathbf{R}_{1} \\mathbf{R}_2 \\cdots  \\mathbf{R}_n = \\mathbf{Q}_1\n",
    "$$\n",
    "where $\\mathbf{R}_j$ are a sequence of upper triangular matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Householder QR does\n",
    "$$\n",
    "    \\mathbf{H}_{p} \\cdots \\mathbf{H}_2 \\mathbf{H}_1 \\mathbf{X} = \\begin{pmatrix} \\mathbf{R}_1 \\\\ \\mathbf{0} \\end{pmatrix},\n",
    "$$\n",
    "where $\\mathbf{H}_j \\in \\mathbf{R}^{n \\times n}$ are a sequence of Householder transformation matrices.\n",
    "\n",
    "    It yields the **full QR** where $\\mathbf{Q} = \\mathbf{H}_1 \\cdots \\mathbf{H}_p \\in \\mathbb{R}^{n \\times n}$. Recall that CGS/MGS only produces the **reduced QR** decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For arbitrary $\\mathbf{x}, \\mathbf{w} \\in \\mathbb{R}^{n}$ with $\\|\\mathbf{x}\\|_2 = \\|\\mathbf{w}\\|_2$, we can construct a **Householder matrix** (or **Householder reflector**)\n",
    "$$\n",
    "    \\mathbf{H} = \\mathbf{I}_n - 2 \\mathbf{u} \\mathbf{u}^T, \\quad \\mathbf{u} = - \\frac{1}{\\|\\mathbf{x} - \\mathbf{w}\\|_2} (\\mathbf{x} - \\mathbf{w}),\n",
    "$$\n",
    "that transforms $\\mathbf{x}$ to $\\mathbf{w}$:\n",
    "$$\n",
    "\t\\mathbf{H} \\mathbf{x} = \\mathbf{w}.\n",
    "$$\n",
    "To see this, note $\\mathbf{u}^T\\mathbf{x} = - (\\mathbf{x}^T\\mathbf{x} - \\mathbf{w}^T\\mathbf{x})/\\Vert \\mathbf{x} - \\mathbf{w} \\Vert_2$. Then\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\mathbf{H}\\mathbf{x} &= \\mathbf{x} + \\frac{2(\\mathbf{x}^T\\mathbf{x} - \\mathbf{w}^T\\mathbf{x})}{\\Vert \\mathbf{x} - \\mathbf{w} \\Vert_2}\\mathbf{u}\n",
    "    = \\mathbf{x} - \\frac{2(\\mathbf{x}^T\\mathbf{x} - \\mathbf{w}^T\\mathbf{x})}{\\Vert \\mathbf{x} - \\mathbf{w} \\Vert_2}\\frac{\\mathbf{x} - \\mathbf{w}}{\\Vert \\mathbf{x} - \\mathbf{w} \\Vert_2}\n",
    "    \\\\\n",
    "    &=\n",
    "    \\frac{(\\Vert\\mathbf{x} - \\mathbf{w}\\Vert_2^2 - 2\\mathbf{x}^T\\mathbf{x} + 2\\mathbf{w}^T\\mathbf{x})\\mathbf{x} + 2(\\mathbf{x}^T\\mathbf{x} - \\mathbf{w}^T\\mathbf{x})\\mathbf{w}}{\\Vert \\mathbf{x} - \\mathbf{w} \\Vert_2^2}\n",
    "    \\\\\n",
    "    &=\n",
    "    \\frac{(\\mathbf{x}^T\\mathbf{x} + \\mathbf{w}^T\\mathbf{w} - 2\\mathbf{w}^T\\mathbf{x} - 2\\mathbf{x}^T\\mathbf{x} + 2\\mathbf{w}^T\\mathbf{x})\\mathbf{x} + 2(\\mathbf{x}^T\\mathbf{x} - \\mathbf{w}^T\\mathbf{x})\\mathbf{w}}{\\Vert \\mathbf{x} - \\mathbf{w} \\Vert_2^2}    \n",
    "    \\\\\n",
    "    &=\n",
    "    \\frac{(\\mathbf{x}^T\\mathbf{x} + \\mathbf{w}^T\\mathbf{w} - 2\\mathbf{w}^T\\mathbf{x})\\mathbf{w}}{\\Vert \\mathbf{x} - \\mathbf{w} \\Vert_2^2}    \n",
    "    \\\\\n",
    "    &= \\frac{\\Vert \\mathbf{x} - \\mathbf{w} \\Vert_2^2\\mathbf{w}}{\\Vert \\mathbf{x} - \\mathbf{w} \\Vert_2^2}    \n",
    "    = \\mathbf{w}\n",
    "\\end{split}\n",
    "$$\n",
    "using $\\mathbf{x}^T\\mathbf{x} = \\mathbf{w}^T\\mathbf{w}$.\n",
    "\n",
    "$\\mathbf{H}$ is symmetric and orthogonal. Calculation of Householder vector $\\mathbf{u}$ costs $3n$ flops for general $\\mathbf{u}$ and $\\mathbf{w}$.\n",
    "\n",
    "<img src=\"https://www.cs.utexas.edu/users/flame/laff/alaff/images/Chapter03/reflector.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "Source: https://www.cs.utexas.edu/users/flame/laff/alaff/images/Chapter03/reflector.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now choose $\\mathbf{H}_1$ so that\n",
    "$$\n",
    "\t\\mathbf{H}_1 \\mathbf{x}_1 = \\begin{pmatrix} \\|\\mathbf{x}_{1}\\|_2 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}.\n",
    "$$\n",
    "That is, $\\mathbf{x} = \\mathbf{x}_1$ and $\\mathbf{w} = \\|\\mathbf{x}_1\\|_2\\mathbf{e}_1$.\n",
    "\n",
    "* Left-multiplying $\\mathbf{H}_1$ zeros out the first column of $\\mathbf{X}$ below (1, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Take $\\mathbf{H}_2$ to zero the second column below diagonal:\n",
    "$$\n",
    "\\mathbf{H}_2\\mathbf{H}_1\\mathbf{X} = \n",
    "\\begin{bmatrix} \n",
    "\\times & \\times & \\times & \\times \\\\ \n",
    "0 & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times}  \\\\\n",
    "0 & \\mathbf{0} & \\boldsymbol{\\times} & \\boldsymbol{\\times}  \\\\\n",
    "0 & \\mathbf{0} & \\boldsymbol{\\times} & \\boldsymbol{\\times}  \\\\\n",
    "0 & \\mathbf{0} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In general, choose the $j$-th Householder transform $\\mathbf{H}_j = \\mathbf{I}_n - 2 \\mathbf{u}_j \\mathbf{u}_j^T$, where \n",
    "$$\n",
    "     \\mathbf{u}_j = \\begin{bmatrix} \\mathbf{0}_{j-1} \\\\ {\\tilde u}_j \\end{bmatrix}, \\quad {\\tilde u}_j \\in \\mathbb{R}^{n-j+1},\n",
    "$$\n",
    "to zero the $j$-th column below diagonal. $\\mathbf{H}_j$ takes the form\n",
    "$$\n",
    "\t\\mathbf{H}_j = \\begin{bmatrix}\n",
    "\t\\mathbf{I}_{j-1} & \\\\\n",
    "\t& \\mathbf{I}_{n-j+1} - 2 {\\tilde u}_j {\\tilde u}_j^T\n",
    "\t\\end{bmatrix} = \\begin{bmatrix}\n",
    "\t\\mathbf{I}_{j-1} & \\\\\n",
    "\t& {\\tilde H}_{j}\n",
    "\t\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Applying a Householder transform $\\mathbf{H} = \\mathbf{I} - 2 \\mathbf{u} \\mathbf{u}^T$ to a matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$\n",
    "$$\n",
    "\t\\mathbf{H} \\mathbf{X} = \\mathbf{X} - 2 \\mathbf{u} (\\mathbf{u}^T \\mathbf{X})\n",
    "$$\n",
    "costs $4np$ flops. **Householder updates never entails explicit formation of the Householder matrices.**\n",
    "\n",
    "* Note applying ${\\tilde H}_j$ to $\\mathbf{X}$ only needs $4(n-j+1)(p-j+1)$ flops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "```r\n",
    "for (j in seq_len(p)) {  # in-place\n",
    "    u <- Householer(X[j:n, j])\n",
    "    for (i in j:p)\n",
    "        X[j:n, i] <- X[j:n, i] - 2 * u * sum(u * X[j:n, i])\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "* The process is done in place. Upper triangular part of $\\mathbf{X}$ is overwritten by $\\mathbf{R}_1$ and the essential Householder vectors ($\\tilde u_{j1}$ is normalized to 1) are stored in $\\mathbf{X}[j:n,j]$.\n",
    "\n",
    "* At $j$-th stage\n",
    "     0. computing the Householder vector ${\\tilde u}_j$ costs $3(n-j+1)$ flops\n",
    "     0. applying the Householder transform ${\\tilde H}_j$ to the $\\mathbf{X}[j:n, j:p]$ block costs $4(n-j+1)(p-j+1)$ flops  \n",
    "     \n",
    "* In total we need $\\sum_{j=1}^p [3(n-j+1) + 4(n-j+1)(p-j+1)] \\approx 2np^2 - \\frac 23 p^3$ flops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Where is $\\mathbf{Q}$? \n",
    "    - $\\mathbf{Q} = \\mathbf{H}_1 \\cdots \\mathbf{H}_p$. In some applications, it's necessary to form the orthogonal matrix $\\mathbf{Q}$. \n",
    "\n",
    "    Accumulating $\\mathbf{Q}$ costs another $2np^2 - \\frac 23 p^3$ flops.\n",
    "\n",
    "* When computing $\\mathbf{Q}^T \\mathbf{v}$ or $\\mathbf{Q} \\mathbf{v}$ as in some applications (e.g., solve linear equation using QR), no need to form $\\mathbf{Q}$. Simply apply Householder transforms successively to the vector $\\mathbf{v}$. (HW)\n",
    "\n",
    "* Computational cost of Householder QR for linear regression: $2n p^2 - \\frac 23 p^3$ (regression coefficients and $\\hat \\sigma^2$) or more (fitted values, s.e., ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "* R function: [`qr`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/qr.html)\n",
    "\n",
    "* Wraps LAPACK routine [`dgeqp3`](http://www.netlib.org/lapack/explore-html/dd/d9a/group__double_g_ecomputational_ga1b0500f49e03d2771b797c6e88adabbb.html)  (with `LAPACK=TRUE`; default uses LINPACK, an ancient version of LAPACK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  # to recall\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(lm(y ~ X - 1)) # least squares solution by QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as\n",
    "qr.solve(X, y)  # or solve(qr(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(qrobj <- qr(X, LAPACK=TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `$qr`: a matrix of same size as input matrix \n",
    "- `$rank`: rank of the input matrix\n",
    "- `$pivot`: pivot vector (for rank-deficient `X`)\n",
    "- `$aux`: normalizing constants of Householder vectors\n",
    "\n",
    "The upper triangular part of `qrobj$qr` contains the $\\mathbf{R}$ of the decomposition and the lower triangular part contains information on the $\\mathbf{Q}$ of the decomposition (stored in compact form using $\\mathbf{Q} = \\mathbf{H}_1 \\cdots \\mathbf{H}_p$, HW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr.Q(qrobj)  # extract Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr.R(qrobj) # extract R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr.qty(qrobj, y)  # this uses the \"compact form\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t(qr.Q(qrobj)) %*% y  # waste of resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(qr.R(qrobj), qr.qty(qrobj, y)[1:p])  # solves R * beta = Q' * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xchol <- Matrix::chol(Matrix::symmpart(t(X) %*% X))  # solution using Cholesky of X'X\n",
    "solve(Xchol, solve(t(Xchol), t(X) %*% y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get back to the odd `A` and `B` matrices that fooled Gram-Schmidt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrA <- qr(A, LAPACK=TRUE)\n",
    "qr.Q(qrA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t(qr.Q(qrA)) %*% qr.Q(qrA)   # orthogonality preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrB <- qr(B, LAPACK=TRUE)\n",
    "qr.Q(qrB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t(qr.Q(qrB)) %*% qr.Q(qrB)  # orthogonality preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR by Givens rotation\n",
    "\n",
    "* Householder transform $\\mathbf{H}_j$ introduces batch of zeros into a vector.\n",
    "\n",
    "* **Givens transform** (aka **Givens rotation**, **Jacobi rotation**, **plane rotation**) selectively zeros one element of a vector.\n",
    "\n",
    "* Overall QR by Givens rotation is less efficient than the Householder method, but is better suited for matrices with structured patterns of nonzero elements.\n",
    "\n",
    "* **Givens/Jacobi rotations**: \n",
    "$$\n",
    "\t\\mathbf{G}(i,k,\\theta) = \\begin{bmatrix} \n",
    "\t1 & & 0 & & 0 & & 0 \\\\\n",
    "\t\\vdots & \\ddots & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "\t0 & & c & & s & & 0 \\\\ \n",
    "\t\\vdots & & \\vdots & \\ddots & \\vdots & & \\vdots \\\\\n",
    "\t0 & & - s & & c & & 0 \\\\\n",
    "\t\\vdots & & \\vdots & & \\vdots & \\ddots & \\vdots \\\\\n",
    "\t0 & & 0 & & 0 & & 1 \\end{bmatrix},\n",
    "$$\n",
    "where $c = \\cos(\\theta)$ and $s = \\sin(\\theta)$. $\\mathbf{G}(i,k,\\theta)$ is orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pre-multiplication by $\\mathbf{G}(i,k,\\theta)^T$ rotates counterclockwise $\\theta$ radians in the $(i,k)$ coordinate plane. If $\\mathbf{x} \\in \\mathbb{R}^n$ and $\\mathbf{y} = \\mathbf{G}(i,k,\\theta)^T \\mathbf{x}$, then\n",
    "$$\n",
    "\ty_j = \\begin{cases}\n",
    "\tcx_i - s x_k & j = i \\\\\n",
    "\tsx_i + cx_k & j = k \\\\\n",
    "\tx_j & j \\ne i, k\n",
    "\t\\end{cases}.\n",
    "$$\n",
    "Apparently if we choose $\\tan(\\theta) = -x_k / x_i$, or equivalently,\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\tc = \\frac{x_i}{\\sqrt{x_i^2 + x_k^2}}, \\quad s = \\frac{-x_k}{\\sqrt{x_i^2 + x_k^2}},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "then $y_k=0$.\n",
    "\n",
    "* Pre-applying Givens transform $\\mathbf{G}(i,k,\\theta)^T \\in \\mathbb{R}^{n \\times n}$ to a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$ only effects two rows of $\\mathbf{\n",
    "A}$:\n",
    "$$\n",
    "\t\\mathbf{A}([i, k], :) \\gets \\begin{bmatrix} c & s \\\\ -s & c \\end{bmatrix}^T \\mathbf{A}([i, k], :),\n",
    "$$\n",
    "costing $6m$ flops.\n",
    "\n",
    "* Post-applying Givens transform $\\mathbf{G}(i,k,\\theta) \\in \\mathbb{R}^{m \\times m}$ to a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$ only effects two columns of $\\mathbf{A}$:\n",
    "$$\n",
    "\t\\mathbf{A}(:, [i,k]) \\gets \\mathbf{A}(:, [i,k]) \\begin{bmatrix} c & s \\\\ -s & c \\end{bmatrix},\n",
    "$$\n",
    "costing $6n$ flops.\n",
    "\n",
    "* QR by Givens: $\\mathbf{G}_t^T \\cdots \\mathbf{G}_1^T \\mathbf{X} =  \\begin{bmatrix} \\mathbf{R}_1 \\\\ \\mathbf{0} \\end{bmatrix}$.\n",
    "\n",
    "<img src=\"QR_by_Givens.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "* Zeros in $\\mathbf{X}$ can also be introduced row-by-row.\n",
    "\n",
    "* If $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, the total cost is $3np^2 - p^3$ flops and $O(np)$ square roots.\n",
    "\n",
    "* Note each Givens transform can be summarized by a single number, which is stored in the zeroed entry of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning of linear regression by least squares\n",
    "\n",
    "* Recall that for nonsingular linear equation solve, i.e. $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$, the condition number  of the problem with $\\mathbf{b}$ held fixed is equal to the condition number of matrix $\\mathbf{A}$\n",
    "$$\n",
    "    \\kappa(\\mathbf{A}) = \\|\\mathbf{A}\\|\\|\\mathbf{A}^{-1}\\|.\n",
    "$$\n",
    "\n",
    "* If $\\mathbf{A}$ is not square, then its condition number is\n",
    "$$\n",
    "    \\kappa(\\mathbf{A}) = \\|\\mathbf{A}\\|\\|\\mathbf{A}^{\\dagger}\\|,\n",
    "$$\n",
    "where $\\mathbf{A}^{\\dagger}$ is Moore-Penrose pseudoinverse of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Condition number for the least squares problem** $\\mathbf{y} \\approx \\mathbf{X}\\boldsymbol{\\beta}$ is more complicated and depends on the residual. It can be shown that the condition number is\n",
    "$$\n",
    "    \\kappa(\\mathbf{X}) + \\frac{\\kappa(\\mathbf{X})^2\\tan\\theta}{\\eta},\n",
    "$$\n",
    "where\n",
    "\n",
    "$$\n",
    "    \\theta = \\cos^{-1}\\frac{\\|\\mathbf{X}\\boldsymbol{\\beta}\\|}{\\|\\boldsymbol{\\beta}\\|},\n",
    "    \\quad\n",
    "    \\eta = \\frac{\\|\\mathbf{X}\\|\\|\\boldsymbol{\\beta}\\|}{\\|\\mathbf{X}\\boldsymbol{\\beta}\\|}\n",
    "    .\n",
    "$$\n",
    "\n",
    "* So if $\\kappa(\\mathbf{X})$ is large ($\\mathbf{X}$ is close to collinear), the condition number of the least squares problem is dominated by $\\kappa(\\mathbf{X})^2$, unless the residuals are small.\n",
    "    - In this case, *stable* algorithm is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is because the problem is equivalent to the normal equation\n",
    "$$\n",
    "    \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}.\n",
    "$$\n",
    "    \n",
    "* Consider the simple case\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{X} = \\begin{bmatrix}\n",
    "\t1 & 1 \\\\\n",
    "\t10^{-3} & 0 \\\\\n",
    "\t0 & 10^{-3}\n",
    "\t\\end{bmatrix}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "Forming normal equation yields a singular Gramian matrix\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{X}^T \\mathbf{X} = \\begin{bmatrix}\n",
    "\t1 & 1 \\\\\n",
    "\t1 & 1\n",
    "\t\\end{bmatrix}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "if executed with a precision of 6 decimal digits. This has the condition number $\\kappa(\\mathbf{X}^T\\mathbf{X})=\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of linear regression\n",
    "\n",
    "Methods for solving linear regression $\\widehat \\beta = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$:\n",
    "\n",
    "| Method            | Flops                  | Remarks                 | Software | Stability   |\n",
    "| :---------------: | :--------------------: | :---------------------: | :------: | :---------: |\n",
    "| Cholesky          | $np^2 + p^3/3$         |                         |          | unstable |\n",
    "| QR by Householder | $2np^2 - (2/3)p^3$     |                         | R, Julia, MATLAB     | stable      |\n",
    "| QR by MGS         | $2np^2$                | $Q_1$ available         |          | stable      | \n",
    "| QR by SVD         | $4n^2p + 8np^2 + 9p^3$ | $X = UDV^T$             |          | most stable |  \n",
    "\n",
    "Remarks:\n",
    "\n",
    "1. When $n \\gg p$, Cholesky is twice faster than QR and need less space.  \n",
    "2. Cholesky is based on the **Gram matrix** $\\mathbf{X}^T \\mathbf{X}$, which can be dynamically updated with incoming data. They can handle huge $n$, moderate $p$ data sets that cannot fit into memory.  \n",
    "3. QR methods are more stable and produce numerically more accurate solution.  \n",
    "4. MGS appears slower than Householder, but it yields $\\mathbf{Q}_1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **There is simply no such thing as a universal 'gold standard' when it comes to algorithms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underdetermined least squares\n",
    "\n",
    "* When the Gram matrix $\\mathbf{X}^T\\mathbf{X}$ is singular (this happens if $\\text{rank}(\\mathbf{X})<p$, in particular $n < p$), the normal equation $\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}$ is *underdetermined*.\n",
    "\n",
    "* It has *many* solutions unless an additional condition is added.\n",
    "\n",
    "* Typically we require solution $\\mathbf{x}$ has a smallest norm; the solution depends on the choice of the norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example: minimum $\\ell_2$ norm solution\n",
    "$$\n",
    "    \\text{minimize}~\\|\\boldsymbol{\\beta}\\|_2 ~\\text{subject to}~\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\n",
    "$$\n",
    "    - Solution is closed form: $\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^{\\dagger}\\mathbf{y}$, where $\\mathbf{X}^{\\dagger}$ is the Moore-Penrose pseudoinverse of $\\mathbf{X}$: $\\mathbf{X}^{\\dagger} = \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}$ if $n<p$ and $\\mathbf{X}$ has full row rank.\n",
    "    - In fact $\\hat{\\boldsymbol{\\beta}}$ is a limit of the ridge regression: $\\hat{\\boldsymbol{\\beta}} = \\lim_{\\lambda\\to 0}(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example: minimum $\\ell_0$ \"norm\" solution\n",
    "$$\n",
    "    \\text{minimize}~\\|\\boldsymbol{\\beta}\\|_0 ~\\text{subject to}~\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\n",
    "$$\n",
    "    - $\\|\\mathbf{x}\\|_0 \\triangleq \\sum_{i=1}^p \\mathbf{1}_{\\{x_i\\neq 0\\}}$, the \"sparsest\" solution.\n",
    "    - Model selection problem. Combinatorial (intractable if $p$ large).\n",
    "    \n",
    "* Compromise: minimum $\\ell_1$ norm solution    \n",
    "$$\n",
    "    \\text{minimize}~\\|\\boldsymbol{\\beta}\\|_1 ~\\text{subject to}~\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\n",
    "$$\n",
    "    - LASSO.\n",
    "    - Convex optimization problem (linear programming). Tractable.\n",
    "    - Under certain conditions recovers the minimum $\\ell_0$ norm solution."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
